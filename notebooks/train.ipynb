{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Dynamically add the 'src' directory to sys.path\n",
    "project_root = Path().resolve().parent  # Get the parent directory of 'notebooks'\n",
    "src_path = project_root / \"src\"\n",
    "sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tokenizer import ChessTokenizer\n",
    "\n",
    "# from dataa import ChessDataset\n",
    "import gc\n",
    "import chess\n",
    "import numpy as np\n",
    "from model import ChessGPTModel\n",
    "from chessdata import ChessDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = ChessTokenizer()\n",
    "mps_device = torch.device(\"mps\")\n",
    "cpu_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 768\n",
    "seq_len = 256\n",
    "n_layer = 16\n",
    "n_head = 8\n",
    "# vocab_size = tok.vocabulary_size()\n",
    "vocab_size = tok.vocabulary_size() #1000\n",
    "config = GPT2Config(vocab_size=vocab_size, n_positions=seq_len, n_ctx=seq_len, n_embd=hidden_size, n_layer=n_layer, n_head=n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.117824"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model.to(mps_device)\n",
    "\n",
    "num_of_parameters = sum(map(torch.numel, model.parameters()))\n",
    "num_of_parameters / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    accuracy = correct / labels.size(0)\n",
    "    return accuracy\n",
    "\n",
    "def top_k_accuracy(logits, labels, k=5):\n",
    "    topk_values, topk_indices = torch.topk(logits, k, dim=-1)  # [batch_size, k]\n",
    "    correct_in_topk = (topk_indices == labels.unsqueeze(-1)).any(dim=-1)\n",
    "    topk_accuracy = correct_in_topk.float().mean().item()\n",
    "    return topk_accuracy\n",
    "\n",
    "def compute_topk_accuracy(logits: torch.Tensor, labels: torch.Tensor, attention_mask: torch.Tensor, pad_token_id: int = 0, k: int = 1) -> float:\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "    # 1) Flatten all except vocab\n",
    "    logits_flat = logits.view(-1, vocab_size)         # shape: [batch_size*seq_len, vocab_size]\n",
    "    labels_flat = labels.view(-1)                     # shape: [batch_size*seq_len]\n",
    "    mask_flat = attention_mask.view(-1).bool()        # shape: [batch_size*seq_len], True=real token, False=pad\n",
    "\n",
    "    # 2) Further exclude positions where label == pad_token_id\n",
    "    #    We'll combine both conditions into a single boolean mask\n",
    "    valid_positions = mask_flat & (labels_flat != pad_token_id)\n",
    "\n",
    "    if valid_positions.sum() == 0:\n",
    "        # If there are no valid positions, return 0 or NaN\n",
    "        return 0.0\n",
    "\n",
    "    valid_logits = logits_flat[valid_positions]       # shape: [#valid, vocab_size]\n",
    "    valid_labels = labels_flat[valid_positions]       # shape: [#valid]\n",
    "\n",
    "    # 3) Get top-k indices along vocab dimension\n",
    "    topk_values, topk_indices = torch.topk(valid_logits, k, dim=-1)  # [#valid, k]\n",
    "\n",
    "    # 4) Check if the correct label is in top-k predictions\n",
    "    correct_in_topk = (topk_indices == valid_labels.unsqueeze(-1)).any(dim=-1)  # [#valid] bool\n",
    "    accuracy = correct_in_topk.float().mean().item()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Example for classification\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 1.0164, Top-1 Accuracy: 0.8203, Top-5 Accuracy: 0.9335\n",
      "Epoch 1, Batch 2, Loss: 7.2724, Top-1 Accuracy: 0.1123, Top-5 Accuracy: 0.3510\n",
      "Epoch 1, Batch 3, Loss: 6.0269, Top-1 Accuracy: 0.1050, Top-5 Accuracy: 0.3446\n",
      "Epoch 1, Batch 4, Loss: 4.5995, Top-1 Accuracy: 0.0990, Top-5 Accuracy: 0.3216\n",
      "Epoch 1, Batch 5, Loss: 3.9492, Top-1 Accuracy: 0.0986, Top-5 Accuracy: 0.3243\n",
      "Epoch 1, Batch 6, Loss: 3.5850, Top-1 Accuracy: 0.0875, Top-5 Accuracy: 0.3143\n",
      "Epoch 1, Batch 7, Loss: 3.3668, Top-1 Accuracy: 0.1008, Top-5 Accuracy: 0.3423\n",
      "Epoch 1, Batch 8, Loss: 3.1903, Top-1 Accuracy: 0.1109, Top-5 Accuracy: 0.3641\n",
      "Epoch 1, Batch 9, Loss: 3.0954, Top-1 Accuracy: 0.1352, Top-5 Accuracy: 0.4013\n",
      "Epoch 1, Batch 10, Loss: 3.0108, Top-1 Accuracy: 0.1426, Top-5 Accuracy: 0.4198\n",
      "Epoch 1, Batch 11, Loss: 3.2845, Top-1 Accuracy: 0.1168, Top-5 Accuracy: 0.3719\n",
      "Epoch 1, Batch 12, Loss: 3.0997, Top-1 Accuracy: 0.1273, Top-5 Accuracy: 0.4177\n",
      "Epoch 1, Batch 13, Loss: 3.1335, Top-1 Accuracy: 0.1309, Top-5 Accuracy: 0.4379\n",
      "Epoch 1, Batch 14, Loss: 3.2803, Top-1 Accuracy: 0.1190, Top-5 Accuracy: 0.3940\n",
      "Epoch 1, Batch 15, Loss: 3.1594, Top-1 Accuracy: 0.1574, Top-5 Accuracy: 0.4213\n",
      "Epoch 1, Batch 16, Loss: 3.2586, Top-1 Accuracy: 0.1250, Top-5 Accuracy: 0.4213\n",
      "Epoch 1, Batch 17, Loss: 3.1622, Top-1 Accuracy: 0.1354, Top-5 Accuracy: 0.4213\n",
      "Epoch 1, Batch 18, Loss: 3.2897, Top-1 Accuracy: 0.1220, Top-5 Accuracy: 0.3978\n",
      "Epoch 1, Batch 19, Loss: 3.3089, Top-1 Accuracy: 0.1189, Top-5 Accuracy: 0.3994\n",
      "Epoch 1, Batch 20, Loss: 3.1669, Top-1 Accuracy: 0.1309, Top-5 Accuracy: 0.4244\n",
      "Epoch 1, Batch 21, Loss: 3.1953, Top-1 Accuracy: 0.1196, Top-5 Accuracy: 0.4117\n",
      "Epoch 1, Batch 22, Loss: 3.2352, Top-1 Accuracy: 0.1408, Top-5 Accuracy: 0.4022\n",
      "Epoch 1, Batch 23, Loss: 3.2760, Top-1 Accuracy: 0.1331, Top-5 Accuracy: 0.3777\n",
      "Epoch 1, Batch 24, Loss: 3.1360, Top-1 Accuracy: 0.1397, Top-5 Accuracy: 0.4305\n",
      "Epoch 1, Batch 25, Loss: 3.2358, Top-1 Accuracy: 0.1144, Top-5 Accuracy: 0.4004\n",
      "Epoch 1, Batch 26, Loss: 3.1586, Top-1 Accuracy: 0.1240, Top-5 Accuracy: 0.4226\n",
      "Epoch 1, Batch 27, Loss: 3.1630, Top-1 Accuracy: 0.1229, Top-5 Accuracy: 0.4089\n",
      "Epoch 1, Batch 28, Loss: 3.1768, Top-1 Accuracy: 0.1219, Top-5 Accuracy: 0.4122\n",
      "Epoch 1, Batch 29, Loss: 3.1520, Top-1 Accuracy: 0.1250, Top-5 Accuracy: 0.4142\n",
      "Epoch 1, Batch 30, Loss: 3.0948, Top-1 Accuracy: 0.1405, Top-5 Accuracy: 0.4343\n",
      "Epoch 1, Batch 31, Loss: 3.1708, Top-1 Accuracy: 0.1359, Top-5 Accuracy: 0.3963\n",
      "Epoch 1, Batch 32, Loss: 3.0367, Top-1 Accuracy: 0.1359, Top-5 Accuracy: 0.4475\n",
      "Epoch 1, Batch 33, Loss: 3.0889, Top-1 Accuracy: 0.1362, Top-5 Accuracy: 0.4261\n",
      "Epoch 1, Batch 34, Loss: 3.2145, Top-1 Accuracy: 0.1208, Top-5 Accuracy: 0.3785\n",
      "Epoch 1, Batch 35, Loss: 3.0908, Top-1 Accuracy: 0.1366, Top-5 Accuracy: 0.4099\n",
      "Epoch 1, Batch 36, Loss: 3.1340, Top-1 Accuracy: 0.1256, Top-5 Accuracy: 0.4079\n",
      "Epoch 1, Batch 37, Loss: 3.0888, Top-1 Accuracy: 0.1382, Top-5 Accuracy: 0.4058\n",
      "Epoch 1, Batch 38, Loss: 3.1218, Top-1 Accuracy: 0.1348, Top-5 Accuracy: 0.4152\n",
      "Epoch 1, Batch 39, Loss: 3.1041, Top-1 Accuracy: 0.1317, Top-5 Accuracy: 0.4182\n",
      "Epoch 1, Batch 40, Loss: 3.1658, Top-1 Accuracy: 0.1129, Top-5 Accuracy: 0.4067\n",
      "Epoch 1, Batch 41, Loss: 3.0215, Top-1 Accuracy: 0.1573, Top-5 Accuracy: 0.4468\n",
      "Epoch 1, Batch 42, Loss: 3.0887, Top-1 Accuracy: 0.1318, Top-5 Accuracy: 0.4124\n",
      "Epoch 1, Batch 43, Loss: 3.1348, Top-1 Accuracy: 0.1321, Top-5 Accuracy: 0.3942\n",
      "Epoch 1, Batch 44, Loss: 3.0748, Top-1 Accuracy: 0.1289, Top-5 Accuracy: 0.4356\n",
      "Epoch 1, Batch 45, Loss: 3.1488, Top-1 Accuracy: 0.1235, Top-5 Accuracy: 0.3919\n",
      "Epoch 1, Batch 46, Loss: 3.2047, Top-1 Accuracy: 0.1279, Top-5 Accuracy: 0.3784\n",
      "Epoch 1, Batch 47, Loss: 3.0366, Top-1 Accuracy: 0.1476, Top-5 Accuracy: 0.4461\n",
      "Epoch 1, Batch 48, Loss: 3.1760, Top-1 Accuracy: 0.1293, Top-5 Accuracy: 0.4148\n",
      "Epoch 1, Batch 49, Loss: 3.0782, Top-1 Accuracy: 0.1414, Top-5 Accuracy: 0.4359\n",
      "Epoch 1, Batch 50, Loss: 3.0307, Top-1 Accuracy: 0.1532, Top-5 Accuracy: 0.4324\n",
      "Epoch 1, Batch 51, Loss: 3.0677, Top-1 Accuracy: 0.1168, Top-5 Accuracy: 0.4085\n",
      "Epoch 1, Batch 52, Loss: 3.0034, Top-1 Accuracy: 0.1588, Top-5 Accuracy: 0.4584\n",
      "Epoch 1, Batch 53, Loss: 3.1717, Top-1 Accuracy: 0.1393, Top-5 Accuracy: 0.4166\n",
      "Epoch 1, Batch 54, Loss: 3.0328, Top-1 Accuracy: 0.1493, Top-5 Accuracy: 0.4267\n",
      "Epoch 1, Batch 55, Loss: 3.0350, Top-1 Accuracy: 0.1590, Top-5 Accuracy: 0.4362\n",
      "Epoch 1, Batch 56, Loss: 3.0615, Top-1 Accuracy: 0.1443, Top-5 Accuracy: 0.4218\n",
      "Epoch 1, Batch 57, Loss: 3.1391, Top-1 Accuracy: 0.1276, Top-5 Accuracy: 0.3995\n",
      "Epoch 1, Batch 58, Loss: 3.0582, Top-1 Accuracy: 0.1484, Top-5 Accuracy: 0.4453\n",
      "Epoch 1, Batch 59, Loss: 3.1155, Top-1 Accuracy: 0.1426, Top-5 Accuracy: 0.4174\n",
      "Epoch 1, Batch 60, Loss: 3.0209, Top-1 Accuracy: 0.1222, Top-5 Accuracy: 0.4475\n",
      "Epoch 1, Batch 61, Loss: 3.0428, Top-1 Accuracy: 0.1356, Top-5 Accuracy: 0.4209\n",
      "Epoch 1, Batch 62, Loss: 3.0767, Top-1 Accuracy: 0.1290, Top-5 Accuracy: 0.4117\n",
      "Epoch 1, Batch 63, Loss: 2.9912, Top-1 Accuracy: 0.1429, Top-5 Accuracy: 0.4413\n",
      "Epoch 1, Batch 64, Loss: 3.1636, Top-1 Accuracy: 0.1391, Top-5 Accuracy: 0.4016\n",
      "Epoch 1, Batch 65, Loss: 2.9352, Top-1 Accuracy: 0.1589, Top-5 Accuracy: 0.4511\n",
      "Epoch 1, Batch 66, Loss: 3.0691, Top-1 Accuracy: 0.1390, Top-5 Accuracy: 0.4220\n",
      "Epoch 2, Batch 1, Loss: 2.7914, Top-1 Accuracy: 0.1914, Top-5 Accuracy: 0.5139\n",
      "Epoch 2, Batch 2, Loss: 2.9966, Top-1 Accuracy: 0.1537, Top-5 Accuracy: 0.4446\n",
      "Epoch 2, Batch 3, Loss: 3.0805, Top-1 Accuracy: 0.1499, Top-5 Accuracy: 0.4225\n",
      "Epoch 2, Batch 4, Loss: 3.1296, Top-1 Accuracy: 0.1508, Top-5 Accuracy: 0.4310\n",
      "Epoch 2, Batch 5, Loss: 3.0938, Top-1 Accuracy: 0.1264, Top-5 Accuracy: 0.4251\n",
      "Epoch 2, Batch 6, Loss: 3.1545, Top-1 Accuracy: 0.1250, Top-5 Accuracy: 0.3863\n",
      "Epoch 2, Batch 7, Loss: 3.0503, Top-1 Accuracy: 0.1415, Top-5 Accuracy: 0.4175\n",
      "Epoch 2, Batch 8, Loss: 2.9787, Top-1 Accuracy: 0.1407, Top-5 Accuracy: 0.4259\n",
      "Epoch 2, Batch 9, Loss: 2.9422, Top-1 Accuracy: 0.1595, Top-5 Accuracy: 0.4425\n",
      "Epoch 2, Batch 10, Loss: 2.9146, Top-1 Accuracy: 0.1575, Top-5 Accuracy: 0.4419\n",
      "Epoch 2, Batch 11, Loss: 3.1229, Top-1 Accuracy: 0.1246, Top-5 Accuracy: 0.4200\n",
      "Epoch 2, Batch 12, Loss: 2.9646, Top-1 Accuracy: 0.1466, Top-5 Accuracy: 0.4306\n",
      "Epoch 2, Batch 13, Loss: 2.9279, Top-1 Accuracy: 0.1619, Top-5 Accuracy: 0.4609\n",
      "Epoch 2, Batch 14, Loss: 3.0625, Top-1 Accuracy: 0.1365, Top-5 Accuracy: 0.4252\n",
      "Epoch 2, Batch 15, Loss: 2.9480, Top-1 Accuracy: 0.1720, Top-5 Accuracy: 0.4686\n",
      "Epoch 2, Batch 16, Loss: 3.0222, Top-1 Accuracy: 0.1402, Top-5 Accuracy: 0.4441\n",
      "Epoch 2, Batch 17, Loss: 2.9206, Top-1 Accuracy: 0.1470, Top-5 Accuracy: 0.4390\n",
      "Epoch 2, Batch 18, Loss: 3.0953, Top-1 Accuracy: 0.1413, Top-5 Accuracy: 0.4087\n",
      "Epoch 2, Batch 19, Loss: 3.0531, Top-1 Accuracy: 0.1406, Top-5 Accuracy: 0.4114\n",
      "Epoch 2, Batch 20, Loss: 2.9708, Top-1 Accuracy: 0.1467, Top-5 Accuracy: 0.4372\n",
      "Epoch 2, Batch 21, Loss: 2.9766, Top-1 Accuracy: 0.1330, Top-5 Accuracy: 0.4378\n",
      "Epoch 2, Batch 22, Loss: 3.0264, Top-1 Accuracy: 0.1674, Top-5 Accuracy: 0.4296\n",
      "Epoch 2, Batch 23, Loss: 3.1156, Top-1 Accuracy: 0.1339, Top-5 Accuracy: 0.3893\n",
      "Epoch 2, Batch 24, Loss: 2.9486, Top-1 Accuracy: 0.1492, Top-5 Accuracy: 0.4444\n",
      "Epoch 2, Batch 25, Loss: 3.0046, Top-1 Accuracy: 0.1405, Top-5 Accuracy: 0.4308\n",
      "Epoch 2, Batch 26, Loss: 2.9947, Top-1 Accuracy: 0.1557, Top-5 Accuracy: 0.4517\n",
      "Epoch 2, Batch 27, Loss: 2.9699, Top-1 Accuracy: 0.1506, Top-5 Accuracy: 0.4505\n",
      "Epoch 2, Batch 28, Loss: 2.9388, Top-1 Accuracy: 0.1668, Top-5 Accuracy: 0.4643\n",
      "Epoch 2, Batch 29, Loss: 2.9757, Top-1 Accuracy: 0.1670, Top-5 Accuracy: 0.4553\n",
      "Epoch 2, Batch 30, Loss: 2.9463, Top-1 Accuracy: 0.1601, Top-5 Accuracy: 0.4728\n",
      "Epoch 2, Batch 31, Loss: 2.9975, Top-1 Accuracy: 0.1562, Top-5 Accuracy: 0.4519\n",
      "Epoch 2, Batch 32, Loss: 2.8580, Top-1 Accuracy: 0.1690, Top-5 Accuracy: 0.4857\n",
      "Epoch 2, Batch 33, Loss: 2.9298, Top-1 Accuracy: 0.1580, Top-5 Accuracy: 0.4523\n",
      "Epoch 2, Batch 34, Loss: 3.0740, Top-1 Accuracy: 0.1230, Top-5 Accuracy: 0.4076\n",
      "Epoch 2, Batch 35, Loss: 2.9243, Top-1 Accuracy: 0.1591, Top-5 Accuracy: 0.4482\n",
      "Epoch 2, Batch 36, Loss: 2.9777, Top-1 Accuracy: 0.1463, Top-5 Accuracy: 0.4182\n",
      "Epoch 2, Batch 37, Loss: 2.9827, Top-1 Accuracy: 0.1470, Top-5 Accuracy: 0.4329\n",
      "Epoch 2, Batch 38, Loss: 2.9946, Top-1 Accuracy: 0.1447, Top-5 Accuracy: 0.4326\n",
      "Epoch 2, Batch 39, Loss: 2.9625, Top-1 Accuracy: 0.1347, Top-5 Accuracy: 0.4315\n",
      "Epoch 2, Batch 40, Loss: 3.0140, Top-1 Accuracy: 0.1415, Top-5 Accuracy: 0.4174\n",
      "Epoch 2, Batch 41, Loss: 2.8862, Top-1 Accuracy: 0.1746, Top-5 Accuracy: 0.4684\n",
      "Epoch 2, Batch 42, Loss: 2.9304, Top-1 Accuracy: 0.1652, Top-5 Accuracy: 0.4373\n",
      "Epoch 2, Batch 43, Loss: 3.0053, Top-1 Accuracy: 0.1349, Top-5 Accuracy: 0.4274\n",
      "Epoch 2, Batch 44, Loss: 2.9605, Top-1 Accuracy: 0.1735, Top-5 Accuracy: 0.4507\n",
      "Epoch 2, Batch 45, Loss: 3.0479, Top-1 Accuracy: 0.1373, Top-5 Accuracy: 0.4152\n",
      "Epoch 2, Batch 46, Loss: 3.0808, Top-1 Accuracy: 0.1372, Top-5 Accuracy: 0.3989\n",
      "Epoch 2, Batch 47, Loss: 2.9088, Top-1 Accuracy: 0.1679, Top-5 Accuracy: 0.4758\n",
      "Epoch 2, Batch 48, Loss: 3.0505, Top-1 Accuracy: 0.1562, Top-5 Accuracy: 0.4338\n",
      "Epoch 2, Batch 49, Loss: 2.9560, Top-1 Accuracy: 0.1531, Top-5 Accuracy: 0.4641\n",
      "Epoch 2, Batch 50, Loss: 2.9059, Top-1 Accuracy: 0.1659, Top-5 Accuracy: 0.4550\n",
      "Epoch 2, Batch 51, Loss: 2.9660, Top-1 Accuracy: 0.1325, Top-5 Accuracy: 0.4337\n",
      "Epoch 2, Batch 52, Loss: 2.8872, Top-1 Accuracy: 0.1588, Top-5 Accuracy: 0.4679\n",
      "Epoch 2, Batch 53, Loss: 3.0255, Top-1 Accuracy: 0.1616, Top-5 Accuracy: 0.4389\n",
      "Epoch 2, Batch 54, Loss: 2.8992, Top-1 Accuracy: 0.1759, Top-5 Accuracy: 0.4519\n",
      "Epoch 2, Batch 55, Loss: 2.8855, Top-1 Accuracy: 0.1845, Top-5 Accuracy: 0.4796\n",
      "Epoch 2, Batch 56, Loss: 2.9396, Top-1 Accuracy: 0.1675, Top-5 Accuracy: 0.4519\n",
      "Epoch 2, Batch 57, Loss: 3.0105, Top-1 Accuracy: 0.1468, Top-5 Accuracy: 0.4213\n",
      "Epoch 2, Batch 58, Loss: 2.9062, Top-1 Accuracy: 0.1617, Top-5 Accuracy: 0.4834\n",
      "Epoch 2, Batch 59, Loss: 2.9879, Top-1 Accuracy: 0.1532, Top-5 Accuracy: 0.4482\n",
      "Epoch 2, Batch 60, Loss: 2.8873, Top-1 Accuracy: 0.1463, Top-5 Accuracy: 0.4794\n",
      "Epoch 2, Batch 61, Loss: 2.9287, Top-1 Accuracy: 0.1568, Top-5 Accuracy: 0.4407\n",
      "Epoch 2, Batch 62, Loss: 2.9780, Top-1 Accuracy: 0.1378, Top-5 Accuracy: 0.4352\n",
      "Epoch 2, Batch 63, Loss: 2.8995, Top-1 Accuracy: 0.1595, Top-5 Accuracy: 0.4640\n",
      "Epoch 2, Batch 64, Loss: 3.0598, Top-1 Accuracy: 0.1473, Top-5 Accuracy: 0.4181\n",
      "Epoch 2, Batch 65, Loss: 2.8318, Top-1 Accuracy: 0.1737, Top-5 Accuracy: 0.4717\n",
      "Epoch 2, Batch 66, Loss: 2.9666, Top-1 Accuracy: 0.1560, Top-5 Accuracy: 0.4390\n",
      "Epoch 3, Batch 1, Loss: 2.7364, Top-1 Accuracy: 0.2120, Top-5 Accuracy: 0.5337\n",
      "Epoch 3, Batch 2, Loss: 2.9285, Top-1 Accuracy: 0.1607, Top-5 Accuracy: 0.4649\n",
      "Epoch 3, Batch 3, Loss: 2.9657, Top-1 Accuracy: 0.1634, Top-5 Accuracy: 0.4564\n",
      "Epoch 3, Batch 4, Loss: 3.0130, Top-1 Accuracy: 0.1612, Top-5 Accuracy: 0.4493\n",
      "Epoch 3, Batch 5, Loss: 2.9859, Top-1 Accuracy: 0.1475, Top-5 Accuracy: 0.4485\n",
      "Epoch 3, Batch 6, Loss: 3.1118, Top-1 Accuracy: 0.1214, Top-5 Accuracy: 0.3970\n",
      "Epoch 3, Batch 7, Loss: 2.9613, Top-1 Accuracy: 0.1573, Top-5 Accuracy: 0.4500\n",
      "Epoch 3, Batch 8, Loss: 2.8747, Top-1 Accuracy: 0.1690, Top-5 Accuracy: 0.4676\n",
      "Epoch 3, Batch 9, Loss: 2.7962, Top-1 Accuracy: 0.1755, Top-5 Accuracy: 0.5172\n",
      "Epoch 3, Batch 10, Loss: 2.8024, Top-1 Accuracy: 0.1696, Top-5 Accuracy: 0.4804\n",
      "Epoch 3, Batch 11, Loss: 3.0488, Top-1 Accuracy: 0.1423, Top-5 Accuracy: 0.4426\n",
      "Epoch 3, Batch 12, Loss: 2.8935, Top-1 Accuracy: 0.1609, Top-5 Accuracy: 0.4499\n",
      "Epoch 3, Batch 13, Loss: 2.8450, Top-1 Accuracy: 0.1795, Top-5 Accuracy: 0.4696\n",
      "Epoch 3, Batch 14, Loss: 2.9802, Top-1 Accuracy: 0.1437, Top-5 Accuracy: 0.4265\n",
      "Epoch 3, Batch 15, Loss: 2.8689, Top-1 Accuracy: 0.1917, Top-5 Accuracy: 0.4729\n",
      "Epoch 3, Batch 16, Loss: 2.9307, Top-1 Accuracy: 0.1533, Top-5 Accuracy: 0.4703\n",
      "Epoch 3, Batch 17, Loss: 2.8480, Top-1 Accuracy: 0.1689, Top-5 Accuracy: 0.4585\n",
      "Epoch 3, Batch 18, Loss: 3.0318, Top-1 Accuracy: 0.1539, Top-5 Accuracy: 0.4306\n",
      "Epoch 3, Batch 19, Loss: 3.0037, Top-1 Accuracy: 0.1497, Top-5 Accuracy: 0.4177\n",
      "Epoch 3, Batch 20, Loss: 2.9064, Top-1 Accuracy: 0.1535, Top-5 Accuracy: 0.4582\n",
      "Epoch 3, Batch 21, Loss: 2.8974, Top-1 Accuracy: 0.1425, Top-5 Accuracy: 0.4727\n",
      "Epoch 3, Batch 22, Loss: 2.9473, Top-1 Accuracy: 0.1886, Top-5 Accuracy: 0.4538\n",
      "Epoch 3, Batch 23, Loss: 3.0352, Top-1 Accuracy: 0.1454, Top-5 Accuracy: 0.4190\n",
      "Epoch 3, Batch 24, Loss: 2.8916, Top-1 Accuracy: 0.1599, Top-5 Accuracy: 0.4608\n",
      "Epoch 3, Batch 25, Loss: 2.9586, Top-1 Accuracy: 0.1540, Top-5 Accuracy: 0.4379\n",
      "Epoch 3, Batch 26, Loss: 2.9182, Top-1 Accuracy: 0.1660, Top-5 Accuracy: 0.4619\n",
      "Epoch 3, Batch 27, Loss: 2.9093, Top-1 Accuracy: 0.1651, Top-5 Accuracy: 0.4597\n",
      "Epoch 3, Batch 28, Loss: 2.8969, Top-1 Accuracy: 0.1772, Top-5 Accuracy: 0.4635\n",
      "Epoch 3, Batch 29, Loss: 2.9163, Top-1 Accuracy: 0.1743, Top-5 Accuracy: 0.4617\n",
      "Epoch 3, Batch 30, Loss: 2.8764, Top-1 Accuracy: 0.1692, Top-5 Accuracy: 0.4721\n",
      "Epoch 3, Batch 31, Loss: 2.9254, Top-1 Accuracy: 0.1712, Top-5 Accuracy: 0.4766\n",
      "Epoch 3, Batch 32, Loss: 2.8069, Top-1 Accuracy: 0.1859, Top-5 Accuracy: 0.5062\n",
      "Epoch 3, Batch 33, Loss: 2.8666, Top-1 Accuracy: 0.1719, Top-5 Accuracy: 0.4778\n",
      "Epoch 3, Batch 34, Loss: 3.0259, Top-1 Accuracy: 0.1376, Top-5 Accuracy: 0.4134\n",
      "Epoch 3, Batch 35, Loss: 2.8689, Top-1 Accuracy: 0.1591, Top-5 Accuracy: 0.4667\n",
      "Epoch 3, Batch 36, Loss: 2.9208, Top-1 Accuracy: 0.1560, Top-5 Accuracy: 0.4431\n",
      "Epoch 3, Batch 37, Loss: 2.9232, Top-1 Accuracy: 0.1579, Top-5 Accuracy: 0.4573\n",
      "Epoch 3, Batch 38, Loss: 2.9162, Top-1 Accuracy: 0.1704, Top-5 Accuracy: 0.4599\n",
      "Epoch 3, Batch 39, Loss: 2.8956, Top-1 Accuracy: 0.1577, Top-5 Accuracy: 0.4732\n",
      "Epoch 3, Batch 40, Loss: 2.9575, Top-1 Accuracy: 0.1523, Top-5 Accuracy: 0.4325\n",
      "Epoch 3, Batch 41, Loss: 2.8399, Top-1 Accuracy: 0.1767, Top-5 Accuracy: 0.4921\n",
      "Epoch 3, Batch 42, Loss: 2.8792, Top-1 Accuracy: 0.1738, Top-5 Accuracy: 0.4665\n",
      "Epoch 3, Batch 43, Loss: 2.9506, Top-1 Accuracy: 0.1528, Top-5 Accuracy: 0.4495\n",
      "Epoch 3, Batch 44, Loss: 2.8895, Top-1 Accuracy: 0.1803, Top-5 Accuracy: 0.4701\n",
      "Epoch 3, Batch 45, Loss: 3.0020, Top-1 Accuracy: 0.1421, Top-5 Accuracy: 0.4276\n",
      "Epoch 3, Batch 46, Loss: 3.0122, Top-1 Accuracy: 0.1465, Top-5 Accuracy: 0.4215\n",
      "Epoch 3, Batch 47, Loss: 2.8389, Top-1 Accuracy: 0.1824, Top-5 Accuracy: 0.5030\n",
      "Epoch 3, Batch 48, Loss: 3.0101, Top-1 Accuracy: 0.1609, Top-5 Accuracy: 0.4519\n",
      "Epoch 3, Batch 49, Loss: 2.8986, Top-1 Accuracy: 0.1617, Top-5 Accuracy: 0.4719\n",
      "Epoch 3, Batch 50, Loss: 2.8555, Top-1 Accuracy: 0.1764, Top-5 Accuracy: 0.4887\n",
      "Epoch 3, Batch 51, Loss: 2.9081, Top-1 Accuracy: 0.1510, Top-5 Accuracy: 0.4563\n",
      "Epoch 3, Batch 52, Loss: 2.8509, Top-1 Accuracy: 0.1635, Top-5 Accuracy: 0.4830\n",
      "Epoch 3, Batch 53, Loss: 2.9746, Top-1 Accuracy: 0.1748, Top-5 Accuracy: 0.4560\n",
      "Epoch 3, Batch 54, Loss: 2.8631, Top-1 Accuracy: 0.1786, Top-5 Accuracy: 0.4615\n",
      "Epoch 3, Batch 55, Loss: 2.8451, Top-1 Accuracy: 0.1947, Top-5 Accuracy: 0.4821\n",
      "Epoch 3, Batch 56, Loss: 2.8962, Top-1 Accuracy: 0.1881, Top-5 Accuracy: 0.4579\n",
      "Epoch 3, Batch 57, Loss: 2.9836, Top-1 Accuracy: 0.1587, Top-5 Accuracy: 0.4286\n",
      "Epoch 3, Batch 58, Loss: 2.8607, Top-1 Accuracy: 0.1841, Top-5 Accuracy: 0.5008\n",
      "Epoch 3, Batch 59, Loss: 2.9529, Top-1 Accuracy: 0.1697, Top-5 Accuracy: 0.4474\n",
      "Epoch 3, Batch 60, Loss: 2.8468, Top-1 Accuracy: 0.1580, Top-5 Accuracy: 0.4988\n",
      "Epoch 3, Batch 61, Loss: 2.9037, Top-1 Accuracy: 0.1561, Top-5 Accuracy: 0.4571\n",
      "Epoch 3, Batch 62, Loss: 2.9444, Top-1 Accuracy: 0.1507, Top-5 Accuracy: 0.4504\n",
      "Epoch 3, Batch 63, Loss: 2.8586, Top-1 Accuracy: 0.1636, Top-5 Accuracy: 0.4740\n",
      "Epoch 3, Batch 64, Loss: 3.0264, Top-1 Accuracy: 0.1473, Top-5 Accuracy: 0.4271\n",
      "Epoch 3, Batch 65, Loss: 2.7978, Top-1 Accuracy: 0.1810, Top-5 Accuracy: 0.4820\n",
      "Epoch 3, Batch 66, Loss: 2.9234, Top-1 Accuracy: 0.1690, Top-5 Accuracy: 0.4410\n",
      "Epoch 4, Batch 1, Loss: 2.6922, Top-1 Accuracy: 0.2165, Top-5 Accuracy: 0.5373\n",
      "Epoch 4, Batch 2, Loss: 2.8834, Top-1 Accuracy: 0.1810, Top-5 Accuracy: 0.4633\n",
      "Epoch 4, Batch 3, Loss: 2.9186, Top-1 Accuracy: 0.1702, Top-5 Accuracy: 0.4674\n",
      "Epoch 4, Batch 4, Loss: 2.9703, Top-1 Accuracy: 0.1708, Top-5 Accuracy: 0.4549\n",
      "Epoch 4, Batch 5, Loss: 2.9466, Top-1 Accuracy: 0.1558, Top-5 Accuracy: 0.4612\n",
      "Epoch 4, Batch 6, Loss: 3.0910, Top-1 Accuracy: 0.1333, Top-5 Accuracy: 0.4071\n",
      "Epoch 4, Batch 7, Loss: 2.9182, Top-1 Accuracy: 0.1615, Top-5 Accuracy: 0.4507\n",
      "Epoch 4, Batch 8, Loss: 2.8464, Top-1 Accuracy: 0.1735, Top-5 Accuracy: 0.4713\n",
      "Epoch 4, Batch 9, Loss: 2.7617, Top-1 Accuracy: 0.1948, Top-5 Accuracy: 0.5390\n",
      "Epoch 4, Batch 10, Loss: 2.7631, Top-1 Accuracy: 0.1910, Top-5 Accuracy: 0.5061\n",
      "Epoch 4, Batch 11, Loss: 2.9978, Top-1 Accuracy: 0.1570, Top-5 Accuracy: 0.4603\n",
      "Epoch 4, Batch 12, Loss: 2.8571, Top-1 Accuracy: 0.1667, Top-5 Accuracy: 0.4757\n",
      "Epoch 4, Batch 13, Loss: 2.8023, Top-1 Accuracy: 0.1876, Top-5 Accuracy: 0.4879\n",
      "Epoch 4, Batch 14, Loss: 2.9413, Top-1 Accuracy: 0.1489, Top-5 Accuracy: 0.4421\n",
      "Epoch 4, Batch 15, Loss: 2.8275, Top-1 Accuracy: 0.1978, Top-5 Accuracy: 0.4961\n",
      "Epoch 4, Batch 16, Loss: 2.9030, Top-1 Accuracy: 0.1595, Top-5 Accuracy: 0.4724\n",
      "Epoch 4, Batch 17, Loss: 2.8131, Top-1 Accuracy: 0.1787, Top-5 Accuracy: 0.4835\n",
      "Epoch 4, Batch 18, Loss: 2.9944, Top-1 Accuracy: 0.1573, Top-5 Accuracy: 0.4415\n",
      "Epoch 4, Batch 19, Loss: 2.9536, Top-1 Accuracy: 0.1486, Top-5 Accuracy: 0.4303\n",
      "Epoch 4, Batch 20, Loss: 2.8729, Top-1 Accuracy: 0.1625, Top-5 Accuracy: 0.4718\n",
      "Epoch 4, Batch 21, Loss: 2.8475, Top-1 Accuracy: 0.1663, Top-5 Accuracy: 0.4751\n",
      "Epoch 4, Batch 22, Loss: 2.9232, Top-1 Accuracy: 0.1823, Top-5 Accuracy: 0.4718\n",
      "Epoch 4, Batch 23, Loss: 2.9931, Top-1 Accuracy: 0.1418, Top-5 Accuracy: 0.4327\n",
      "Epoch 4, Batch 24, Loss: 2.8679, Top-1 Accuracy: 0.1719, Top-5 Accuracy: 0.4703\n",
      "Epoch 4, Batch 25, Loss: 2.9238, Top-1 Accuracy: 0.1667, Top-5 Accuracy: 0.4590\n",
      "Epoch 4, Batch 26, Loss: 2.8742, Top-1 Accuracy: 0.1796, Top-5 Accuracy: 0.4842\n",
      "Epoch 4, Batch 27, Loss: 2.8720, Top-1 Accuracy: 0.1816, Top-5 Accuracy: 0.4696\n",
      "Epoch 4, Batch 28, Loss: 2.8571, Top-1 Accuracy: 0.1828, Top-5 Accuracy: 0.4804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 47\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# running_loss += loss.item()\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    443\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "model.train()\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    top_1_accuracy = 0.0\n",
    "    top_5_accuracy = 0.0\n",
    "\n",
    "    dataset = ChessDataset(file_name=\"./data/trainingmedium.pgn\", tokenizer=tok, max_seq_len=seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "    for i, (cur_token_ids, cur_attn_mask, cur_legal_mask, cur_labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cur_attn_mask = cur_attn_mask.to(mps_device)\n",
    "        cur_token_ids = cur_token_ids.to(mps_device)\n",
    "        cur_legal_mask = cur_legal_mask.to(mps_device)\n",
    "        cur_labels = cur_labels.to(mps_device)\n",
    "\n",
    "        # print(cur_token_ids.shape)\n",
    "        # print(cur_attn_mask.shape)\n",
    "        # print(cur_legal_mask.shape)\n",
    "        # print(cur_labels.shape)\n",
    "\n",
    "        outputs = model(input_ids = cur_token_ids, attention_mask = cur_attn_mask).logits\n",
    "        masked_logits = outputs.masked_fill(~cur_legal_mask, float('-1e10'))\n",
    "        vocab_size = masked_logits.size(-1)\n",
    "        \n",
    "        top_1_accuracy += compute_topk_accuracy(masked_logits, cur_labels, cur_attn_mask, k=1)\n",
    "        top_5_accuracy += compute_topk_accuracy(masked_logits, cur_labels, cur_attn_mask, k=5)\n",
    "\n",
    "        masked_logits = masked_logits.view(-1, vocab_size)\n",
    "        labels_flat = cur_labels.view(-1)\n",
    "        loss = loss_fn(masked_logits, labels_flat)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        del outputs, cur_token_ids, cur_attn_mask, cur_legal_mask, cur_labels\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # running_loss += loss.item()\n",
    "        if (epoch + 1) % 1 == 0 and (i + 1) % print_every == 0:\n",
    "            print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {loss / print_every:.4f}, Top-1 Accuracy: {top_1_accuracy / print_every:.4f}, Top-5 Accuracy: {top_5_accuracy / print_every:.4f}')\n",
    "            running_loss = 0.0\n",
    "            top_1_accuracy = 0.0\n",
    "            top_5_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./modelv0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.63404244, 762), (0.30915236, 1032), (0.00054985494, 1302), (0.00013388555, 29), (0.00010157549, 1301), (9.4130475e-05, 1031), (6.9529495e-05, 495), (6.3696054e-05, 223), (5.403922e-05, 761), (5.3789186e-05, 1567), (5.3702457e-05, 1808), (4.0073075e-05, 30), (3.728253e-05, 1530), (3.5871017e-05, 249), (3.153475e-05, 496), (3.1181306e-05, 248), (2.8857305e-05, 213), (2.8393255e-05, 1970), (2.8393255e-05, 1969), (2.8393255e-05, 1968), (2.8393255e-05, 1967), (2.8393255e-05, 1966), (2.8393255e-05, 1965), (2.8393255e-05, 1964), (2.8393255e-05, 1963), (2.8393255e-05, 1962), (2.8393255e-05, 1961), (2.8393255e-05, 1960), (2.8393255e-05, 1959), (2.8393255e-05, 1958), (2.8393255e-05, 1957), (2.8393255e-05, 1956), (2.8393255e-05, 1955), (2.8393255e-05, 1954), (2.8393255e-05, 1953), (2.8393255e-05, 1952), (2.8393255e-05, 1951), (2.8393255e-05, 1950), (2.8393255e-05, 1949), (2.8393255e-05, 1948), (2.8393255e-05, 1947), (2.8393255e-05, 1946), (2.8393255e-05, 1945), (2.8393255e-05, 1944), (2.8393255e-05, 1943), (2.8393255e-05, 1942), (2.8393255e-05, 1941), (2.8393255e-05, 1940), (2.8393255e-05, 1939), (2.8393255e-05, 1938), (2.8393255e-05, 1937), (2.8393255e-05, 1936), (2.8393255e-05, 1935), (2.8393255e-05, 1934), (2.8393255e-05, 1933), (2.8393255e-05, 1932), (2.8393255e-05, 1931), (2.8393255e-05, 1930), (2.8393255e-05, 1929), (2.8393255e-05, 1928), (2.8393255e-05, 1927), (2.8393255e-05, 1926), (2.8393255e-05, 1925), (2.8393255e-05, 1924), (2.8393255e-05, 1923), (2.8393255e-05, 1922), (2.8393255e-05, 1921), (2.8393255e-05, 1920), (2.8393255e-05, 1919), (2.8393255e-05, 1918), (2.8393255e-05, 1917), (2.8393255e-05, 1916), (2.8393255e-05, 1915), (2.8393255e-05, 1914), (2.8393255e-05, 1913), (2.8393255e-05, 1912), (2.8393255e-05, 1911), (2.8393255e-05, 1910), (2.8393255e-05, 1909), (2.8393255e-05, 1908), (2.8393255e-05, 1907), (2.8393255e-05, 1906), (2.8393255e-05, 1905), (2.8393255e-05, 1904), (2.8393255e-05, 1903), (2.8393255e-05, 1902), (2.8393255e-05, 1901), (2.8393255e-05, 1900), (2.8393255e-05, 1899), (2.8393255e-05, 1898), (2.8393255e-05, 1897), (2.8393255e-05, 1896), (2.8393255e-05, 1895), (2.8393255e-05, 1894), (2.8393255e-05, 1893), (2.8393255e-05, 1892), (2.8393255e-05, 1891), (2.8393255e-05, 1890), (2.8393255e-05, 1889), (2.8393255e-05, 1888), (2.8393255e-05, 1887), (2.8393255e-05, 1886), (2.8393255e-05, 1885), (2.8393255e-05, 1884), (2.8393255e-05, 1883), (2.8393255e-05, 1882), (2.8393255e-05, 1881), (2.8393255e-05, 1880), (2.8393255e-05, 1879), (2.8393255e-05, 1878), (2.8393255e-05, 1877), (2.8393255e-05, 1876), (2.8393255e-05, 1875), (2.8393255e-05, 1874), (2.8393255e-05, 1873), (2.8393255e-05, 1872), (2.8393255e-05, 1871), (2.8393255e-05, 1870), (2.8393255e-05, 1869), (2.8393255e-05, 1868), (2.8393255e-05, 1867), (2.8393255e-05, 1866), (2.8393255e-05, 1865), (2.8393255e-05, 1864), (2.8393255e-05, 1863), (2.8393255e-05, 1862), (2.8393255e-05, 1861), (2.8393255e-05, 1860), (2.8393255e-05, 1859), (2.8393255e-05, 1858), (2.8393255e-05, 1857), (2.8393255e-05, 1856), (2.8393255e-05, 1855), (2.8393255e-05, 1854), (2.8393255e-05, 1853), (2.8393255e-05, 1852), (2.8393255e-05, 1851), (2.8393255e-05, 1850), (2.8393255e-05, 1849), (2.8393255e-05, 1848), (2.8393255e-05, 1847), (2.8393255e-05, 1846), (2.8393255e-05, 1845), (2.8393255e-05, 1844), (2.8393255e-05, 1843), (2.8393255e-05, 1842), (2.8393255e-05, 1841), (2.8393255e-05, 1840), (2.8393255e-05, 1839), (2.8393255e-05, 1838), (2.8393255e-05, 1837), (2.8393255e-05, 1836), (2.8393255e-05, 1835), (2.8393255e-05, 1834), (2.8393255e-05, 1833), (2.8393255e-05, 1832), (2.8393255e-05, 1831), (2.8393255e-05, 1830), (2.8393255e-05, 1829), (2.8393255e-05, 1828), (2.8393255e-05, 1827), (2.8393255e-05, 1826), (2.8393255e-05, 1825), (2.8393255e-05, 1824), (2.8393255e-05, 1823), (2.8393255e-05, 1822), (2.8393255e-05, 1821), (2.8393255e-05, 1820), (2.8393255e-05, 1819), (2.8393255e-05, 1818), (2.8393255e-05, 1817), (2.8393255e-05, 1816), (2.8393255e-05, 1815), (2.8393255e-05, 1814), (2.8393255e-05, 1813), (2.8393255e-05, 1812), (2.8393255e-05, 1811), (2.8393255e-05, 1810), (2.8393255e-05, 1807), (2.8393255e-05, 1806), (2.8393255e-05, 1805), (2.8393255e-05, 1804), (2.8393255e-05, 1803), (2.8393255e-05, 1802), (2.8393255e-05, 1801), (2.8393255e-05, 1800), (2.8393255e-05, 1799), (2.8393255e-05, 1798), (2.8393255e-05, 1797), (2.8393255e-05, 1796), (2.8393255e-05, 1795), (2.8393255e-05, 1794), (2.8393255e-05, 1793), (2.8393255e-05, 1792), (2.8393255e-05, 1791), (2.8393255e-05, 1790), (2.8393255e-05, 1789), (2.8393255e-05, 1788), (2.8393255e-05, 1787), (2.8393255e-05, 1786), (2.8393255e-05, 1785), (2.8393255e-05, 1784), (2.8393255e-05, 1783), (2.8393255e-05, 1782), (2.8393255e-05, 1781), (2.8393255e-05, 1780), (2.8393255e-05, 1779), (2.8393255e-05, 1778), (2.8393255e-05, 1777), (2.8393255e-05, 1776), (2.8393255e-05, 1775), (2.8393255e-05, 1774), (2.8393255e-05, 1773), (2.8393255e-05, 1772), (2.8393255e-05, 1771), (2.8393255e-05, 1770), (2.8393255e-05, 1769), (2.8393255e-05, 1768), (2.8393255e-05, 1767), (2.8393255e-05, 1766), (2.8393255e-05, 1765), (2.8393255e-05, 1764), (2.8393255e-05, 1763), (2.8393255e-05, 1762), (2.8393255e-05, 1761), (2.8393255e-05, 1760), (2.8393255e-05, 1759), (2.8393255e-05, 1758), (2.8393255e-05, 1757), (2.8393255e-05, 1756), (2.8393255e-05, 1755), (2.8393255e-05, 1754), (2.8393255e-05, 1753), (2.8393255e-05, 1752), (2.8393255e-05, 1751), (2.8393255e-05, 1750), (2.8393255e-05, 1749), (2.8393255e-05, 1748), (2.8393255e-05, 1747), (2.8393255e-05, 1746), (2.8393255e-05, 1745), (2.8393255e-05, 1744), (2.8393255e-05, 1743), (2.8393255e-05, 1742), (2.8393255e-05, 1741), (2.8393255e-05, 1740), (2.8393255e-05, 1739), (2.8393255e-05, 1738), (2.8393255e-05, 1737), (2.8393255e-05, 1736), (2.8393255e-05, 1735), (2.8393255e-05, 1734), (2.8393255e-05, 1733), (2.8393255e-05, 1732), (2.8393255e-05, 1731), (2.8393255e-05, 1730), (2.8393255e-05, 1729), (2.8393255e-05, 1728), (2.8393255e-05, 1727), (2.8393255e-05, 1726), (2.8393255e-05, 1725), (2.8393255e-05, 1724), (2.8393255e-05, 1723), (2.8393255e-05, 1722), (2.8393255e-05, 1721), (2.8393255e-05, 1720), (2.8393255e-05, 1719), (2.8393255e-05, 1718), (2.8393255e-05, 1717), (2.8393255e-05, 1716), (2.8393255e-05, 1715), (2.8393255e-05, 1714), (2.8393255e-05, 1713), (2.8393255e-05, 1712), (2.8393255e-05, 1711), (2.8393255e-05, 1710), (2.8393255e-05, 1709), (2.8393255e-05, 1708), (2.8393255e-05, 1707), (2.8393255e-05, 1706), (2.8393255e-05, 1705), (2.8393255e-05, 1704), (2.8393255e-05, 1703), (2.8393255e-05, 1702), (2.8393255e-05, 1701), (2.8393255e-05, 1700), (2.8393255e-05, 1699), (2.8393255e-05, 1698), (2.8393255e-05, 1697), (2.8393255e-05, 1696), (2.8393255e-05, 1695), (2.8393255e-05, 1694), (2.8393255e-05, 1693), (2.8393255e-05, 1692), (2.8393255e-05, 1691), (2.8393255e-05, 1690), (2.8393255e-05, 1689), (2.8393255e-05, 1688), (2.8393255e-05, 1687), (2.8393255e-05, 1686), (2.8393255e-05, 1685), (2.8393255e-05, 1684), (2.8393255e-05, 1683), (2.8393255e-05, 1682), (2.8393255e-05, 1681), (2.8393255e-05, 1680), (2.8393255e-05, 1679), (2.8393255e-05, 1678), (2.8393255e-05, 1677), (2.8393255e-05, 1676), (2.8393255e-05, 1675), (2.8393255e-05, 1674), (2.8393255e-05, 1673), (2.8393255e-05, 1672), (2.8393255e-05, 1671), (2.8393255e-05, 1670), (2.8393255e-05, 1669), (2.8393255e-05, 1668), (2.8393255e-05, 1667), (2.8393255e-05, 1666), (2.8393255e-05, 1665), (2.8393255e-05, 1664), (2.8393255e-05, 1663), (2.8393255e-05, 1662), (2.8393255e-05, 1661), (2.8393255e-05, 1660), (2.8393255e-05, 1659), (2.8393255e-05, 1658), (2.8393255e-05, 1657), (2.8393255e-05, 1656), (2.8393255e-05, 1655), (2.8393255e-05, 1654), (2.8393255e-05, 1653), (2.8393255e-05, 1652), (2.8393255e-05, 1651), (2.8393255e-05, 1650), (2.8393255e-05, 1649), (2.8393255e-05, 1648), (2.8393255e-05, 1647), (2.8393255e-05, 1646), (2.8393255e-05, 1645), (2.8393255e-05, 1644), (2.8393255e-05, 1643), (2.8393255e-05, 1642), (2.8393255e-05, 1641), (2.8393255e-05, 1640), (2.8393255e-05, 1639), (2.8393255e-05, 1638), (2.8393255e-05, 1637), (2.8393255e-05, 1636), (2.8393255e-05, 1635), (2.8393255e-05, 1634), (2.8393255e-05, 1633), (2.8393255e-05, 1632), (2.8393255e-05, 1631), (2.8393255e-05, 1630), (2.8393255e-05, 1629), (2.8393255e-05, 1628), (2.8393255e-05, 1627), (2.8393255e-05, 1626), (2.8393255e-05, 1625), (2.8393255e-05, 1624), (2.8393255e-05, 1623), (2.8393255e-05, 1622), (2.8393255e-05, 1621), (2.8393255e-05, 1620), (2.8393255e-05, 1619), (2.8393255e-05, 1618), (2.8393255e-05, 1617), (2.8393255e-05, 1616), (2.8393255e-05, 1615), (2.8393255e-05, 1614), (2.8393255e-05, 1613), (2.8393255e-05, 1612), (2.8393255e-05, 1611), (2.8393255e-05, 1610), (2.8393255e-05, 1609), (2.8393255e-05, 1608), (2.8393255e-05, 1607), (2.8393255e-05, 1606), (2.8393255e-05, 1605), (2.8393255e-05, 1604), (2.8393255e-05, 1603), (2.8393255e-05, 1602), (2.8393255e-05, 1601), (2.8393255e-05, 1600), (2.8393255e-05, 1599), (2.8393255e-05, 1598), (2.8393255e-05, 1597), (2.8393255e-05, 1596), (2.8393255e-05, 1595), (2.8393255e-05, 1594), (2.8393255e-05, 1593), (2.8393255e-05, 1592), (2.8393255e-05, 1591), (2.8393255e-05, 1590), (2.8393255e-05, 1589), (2.8393255e-05, 1588), (2.8393255e-05, 1587), (2.8393255e-05, 1586), (2.8393255e-05, 1585), (2.8393255e-05, 1584), (2.8393255e-05, 1583), (2.8393255e-05, 1582), (2.8393255e-05, 1581), (2.8393255e-05, 1580), (2.8393255e-05, 1579), (2.8393255e-05, 1578), (2.8393255e-05, 1577), (2.8393255e-05, 1576), (2.8393255e-05, 1575), (2.8393255e-05, 1574), (2.8393255e-05, 1573), (2.8393255e-05, 1572), (2.8393255e-05, 1571), (2.8393255e-05, 1570), (2.8393255e-05, 1569), (2.8393255e-05, 1568), (2.8393255e-05, 1565), (2.8393255e-05, 1564), (2.8393255e-05, 1563), (2.8393255e-05, 1562), (2.8393255e-05, 1561), (2.8393255e-05, 1560), (2.8393255e-05, 1559), (2.8393255e-05, 1558), (2.8393255e-05, 1557), (2.8393255e-05, 1556), (2.8393255e-05, 1555), (2.8393255e-05, 1554), (2.8393255e-05, 1553), (2.8393255e-05, 1552), (2.8393255e-05, 1551), (2.8393255e-05, 1550), (2.8393255e-05, 1549), (2.8393255e-05, 1548), (2.8393255e-05, 1547), (2.8393255e-05, 1546), (2.8393255e-05, 1545), (2.8393255e-05, 1544), (2.8393255e-05, 1543), (2.8393255e-05, 1542), (2.8393255e-05, 1541), (2.8393255e-05, 1539), (2.8393255e-05, 1538), (2.8393255e-05, 1537), (2.8393255e-05, 1536), (2.8393255e-05, 1535), (2.8393255e-05, 1534), (2.8393255e-05, 1533), (2.8393255e-05, 1532), (2.8393255e-05, 1531), (2.8393255e-05, 1529), (2.8393255e-05, 1528), (2.8393255e-05, 1527), (2.8393255e-05, 1526), (2.8393255e-05, 1525), (2.8393255e-05, 1524), (2.8393255e-05, 1523), (2.8393255e-05, 1522), (2.8393255e-05, 1521), (2.8393255e-05, 1520), (2.8393255e-05, 1519), (2.8393255e-05, 1518), (2.8393255e-05, 1517), (2.8393255e-05, 1516), (2.8393255e-05, 1515), (2.8393255e-05, 1514), (2.8393255e-05, 1513), (2.8393255e-05, 1512), (2.8393255e-05, 1511), (2.8393255e-05, 1510), (2.8393255e-05, 1509), (2.8393255e-05, 1508), (2.8393255e-05, 1507), (2.8393255e-05, 1506), (2.8393255e-05, 1505), (2.8393255e-05, 1504), (2.8393255e-05, 1503), (2.8393255e-05, 1502), (2.8393255e-05, 1501), (2.8393255e-05, 1500), (2.8393255e-05, 1499), (2.8393255e-05, 1498), (2.8393255e-05, 1497), (2.8393255e-05, 1496), (2.8393255e-05, 1495), (2.8393255e-05, 1494), (2.8393255e-05, 1493), (2.8393255e-05, 1492), (2.8393255e-05, 1491), (2.8393255e-05, 1490), (2.8393255e-05, 1489), (2.8393255e-05, 1488), (2.8393255e-05, 1487), (2.8393255e-05, 1486), (2.8393255e-05, 1485), (2.8393255e-05, 1484), (2.8393255e-05, 1483), (2.8393255e-05, 1482), (2.8393255e-05, 1481), (2.8393255e-05, 1480), (2.8393255e-05, 1479), (2.8393255e-05, 1478), (2.8393255e-05, 1477), (2.8393255e-05, 1476), (2.8393255e-05, 1475), (2.8393255e-05, 1474), (2.8393255e-05, 1473), (2.8393255e-05, 1472), (2.8393255e-05, 1471), (2.8393255e-05, 1470), (2.8393255e-05, 1469), (2.8393255e-05, 1468), (2.8393255e-05, 1467), (2.8393255e-05, 1466), (2.8393255e-05, 1465), (2.8393255e-05, 1464), (2.8393255e-05, 1463), (2.8393255e-05, 1462), (2.8393255e-05, 1461), (2.8393255e-05, 1460), (2.8393255e-05, 1459), (2.8393255e-05, 1458), (2.8393255e-05, 1457), (2.8393255e-05, 1456), (2.8393255e-05, 1455), (2.8393255e-05, 1454), (2.8393255e-05, 1453), (2.8393255e-05, 1452), (2.8393255e-05, 1451), (2.8393255e-05, 1450), (2.8393255e-05, 1449), (2.8393255e-05, 1448), (2.8393255e-05, 1447), (2.8393255e-05, 1446), (2.8393255e-05, 1445), (2.8393255e-05, 1444), (2.8393255e-05, 1443), (2.8393255e-05, 1442), (2.8393255e-05, 1441), (2.8393255e-05, 1440), (2.8393255e-05, 1439), (2.8393255e-05, 1438), (2.8393255e-05, 1437), (2.8393255e-05, 1436), (2.8393255e-05, 1435), (2.8393255e-05, 1434), (2.8393255e-05, 1433), (2.8393255e-05, 1432), (2.8393255e-05, 1431), (2.8393255e-05, 1430), (2.8393255e-05, 1429), (2.8393255e-05, 1428), (2.8393255e-05, 1427), (2.8393255e-05, 1426), (2.8393255e-05, 1425), (2.8393255e-05, 1424), (2.8393255e-05, 1423), (2.8393255e-05, 1422), (2.8393255e-05, 1421), (2.8393255e-05, 1420), (2.8393255e-05, 1419), (2.8393255e-05, 1418), (2.8393255e-05, 1417), (2.8393255e-05, 1416), (2.8393255e-05, 1415), (2.8393255e-05, 1414), (2.8393255e-05, 1413), (2.8393255e-05, 1412), (2.8393255e-05, 1411), (2.8393255e-05, 1410), (2.8393255e-05, 1409), (2.8393255e-05, 1408), (2.8393255e-05, 1407), (2.8393255e-05, 1406), (2.8393255e-05, 1405), (2.8393255e-05, 1404), (2.8393255e-05, 1403), (2.8393255e-05, 1402), (2.8393255e-05, 1401), (2.8393255e-05, 1400), (2.8393255e-05, 1399), (2.8393255e-05, 1398), (2.8393255e-05, 1397), (2.8393255e-05, 1396), (2.8393255e-05, 1395), (2.8393255e-05, 1394), (2.8393255e-05, 1393), (2.8393255e-05, 1392), (2.8393255e-05, 1391), (2.8393255e-05, 1390), (2.8393255e-05, 1389), (2.8393255e-05, 1388), (2.8393255e-05, 1387), (2.8393255e-05, 1386), (2.8393255e-05, 1385), (2.8393255e-05, 1384), (2.8393255e-05, 1383), (2.8393255e-05, 1382), (2.8393255e-05, 1381), (2.8393255e-05, 1380), (2.8393255e-05, 1379), (2.8393255e-05, 1378), (2.8393255e-05, 1377), (2.8393255e-05, 1376), (2.8393255e-05, 1375), (2.8393255e-05, 1374), (2.8393255e-05, 1373), (2.8393255e-05, 1372), (2.8393255e-05, 1371), (2.8393255e-05, 1370), (2.8393255e-05, 1369), (2.8393255e-05, 1368), (2.8393255e-05, 1367), (2.8393255e-05, 1366), (2.8393255e-05, 1365), (2.8393255e-05, 1364), (2.8393255e-05, 1363), (2.8393255e-05, 1362), (2.8393255e-05, 1361), (2.8393255e-05, 1360), (2.8393255e-05, 1359), (2.8393255e-05, 1358), (2.8393255e-05, 1357), (2.8393255e-05, 1356), (2.8393255e-05, 1355), (2.8393255e-05, 1354), (2.8393255e-05, 1353), (2.8393255e-05, 1352), (2.8393255e-05, 1351), (2.8393255e-05, 1350), (2.8393255e-05, 1349), (2.8393255e-05, 1348), (2.8393255e-05, 1347), (2.8393255e-05, 1346), (2.8393255e-05, 1345), (2.8393255e-05, 1344), (2.8393255e-05, 1343), (2.8393255e-05, 1342), (2.8393255e-05, 1341), (2.8393255e-05, 1340), (2.8393255e-05, 1339), (2.8393255e-05, 1338), (2.8393255e-05, 1337), (2.8393255e-05, 1336), (2.8393255e-05, 1335), (2.8393255e-05, 1334), (2.8393255e-05, 1333), (2.8393255e-05, 1332), (2.8393255e-05, 1331), (2.8393255e-05, 1330), (2.8393255e-05, 1329), (2.8393255e-05, 1328), (2.8393255e-05, 1327), (2.8393255e-05, 1326), (2.8393255e-05, 1325), (2.8393255e-05, 1324), (2.8393255e-05, 1323), (2.8393255e-05, 1322), (2.8393255e-05, 1321), (2.8393255e-05, 1320), (2.8393255e-05, 1319), (2.8393255e-05, 1318), (2.8393255e-05, 1317), (2.8393255e-05, 1316), (2.8393255e-05, 1315), (2.8393255e-05, 1314), (2.8393255e-05, 1313), (2.8393255e-05, 1312), (2.8393255e-05, 1311), (2.8393255e-05, 1310), (2.8393255e-05, 1309), (2.8393255e-05, 1308), (2.8393255e-05, 1307), (2.8393255e-05, 1306), (2.8393255e-05, 1305), (2.8393255e-05, 1304), (2.8393255e-05, 1303), (2.8393255e-05, 1300), (2.8393255e-05, 1299), (2.8393255e-05, 1298), (2.8393255e-05, 1297), (2.8393255e-05, 1296), (2.8393255e-05, 1295), (2.8393255e-05, 1294), (2.8393255e-05, 1293), (2.8393255e-05, 1292), (2.8393255e-05, 1291), (2.8393255e-05, 1290), (2.8393255e-05, 1289), (2.8393255e-05, 1288), (2.8393255e-05, 1287), (2.8393255e-05, 1286), (2.8393255e-05, 1285), (2.8393255e-05, 1284), (2.8393255e-05, 1283), (2.8393255e-05, 1282), (2.8393255e-05, 1281), (2.8393255e-05, 1280), (2.8393255e-05, 1279), (2.8393255e-05, 1278), (2.8393255e-05, 1277), (2.8393255e-05, 1276), (2.8393255e-05, 1275), (2.8393255e-05, 1274), (2.8393255e-05, 1273), (2.8393255e-05, 1272), (2.8393255e-05, 1271), (2.8393255e-05, 1270), (2.8393255e-05, 1269), (2.8393255e-05, 1268), (2.8393255e-05, 1267), (2.8393255e-05, 1266), (2.8393255e-05, 1265), (2.8393255e-05, 1264), (2.8393255e-05, 1263), (2.8393255e-05, 1262), (2.8393255e-05, 1261), (2.8393255e-05, 1260), (2.8393255e-05, 1259), (2.8393255e-05, 1258), (2.8393255e-05, 1257), (2.8393255e-05, 1256), (2.8393255e-05, 1255), (2.8393255e-05, 1254), (2.8393255e-05, 1253), (2.8393255e-05, 1252), (2.8393255e-05, 1251), (2.8393255e-05, 1250), (2.8393255e-05, 1249), (2.8393255e-05, 1248), (2.8393255e-05, 1247), (2.8393255e-05, 1246), (2.8393255e-05, 1245), (2.8393255e-05, 1244), (2.8393255e-05, 1243), (2.8393255e-05, 1242), (2.8393255e-05, 1241), (2.8393255e-05, 1240), (2.8393255e-05, 1239), (2.8393255e-05, 1238), (2.8393255e-05, 1237), (2.8393255e-05, 1236), (2.8393255e-05, 1235), (2.8393255e-05, 1234), (2.8393255e-05, 1233), (2.8393255e-05, 1232), (2.8393255e-05, 1231), (2.8393255e-05, 1230), (2.8393255e-05, 1229), (2.8393255e-05, 1228), (2.8393255e-05, 1227), (2.8393255e-05, 1226), (2.8393255e-05, 1225), (2.8393255e-05, 1224), (2.8393255e-05, 1223), (2.8393255e-05, 1222), (2.8393255e-05, 1221), (2.8393255e-05, 1220), (2.8393255e-05, 1219), (2.8393255e-05, 1218), (2.8393255e-05, 1217), (2.8393255e-05, 1216), (2.8393255e-05, 1215), (2.8393255e-05, 1214), (2.8393255e-05, 1213), (2.8393255e-05, 1212), (2.8393255e-05, 1211), (2.8393255e-05, 1210), (2.8393255e-05, 1209), (2.8393255e-05, 1208), (2.8393255e-05, 1207), (2.8393255e-05, 1206), (2.8393255e-05, 1205), (2.8393255e-05, 1204), (2.8393255e-05, 1203), (2.8393255e-05, 1202), (2.8393255e-05, 1201), (2.8393255e-05, 1200), (2.8393255e-05, 1199), (2.8393255e-05, 1198), (2.8393255e-05, 1197), (2.8393255e-05, 1196), (2.8393255e-05, 1195), (2.8393255e-05, 1194), (2.8393255e-05, 1193), (2.8393255e-05, 1192), (2.8393255e-05, 1191), (2.8393255e-05, 1190), (2.8393255e-05, 1189), (2.8393255e-05, 1188), (2.8393255e-05, 1187), (2.8393255e-05, 1186), (2.8393255e-05, 1185), (2.8393255e-05, 1184), (2.8393255e-05, 1183), (2.8393255e-05, 1182), (2.8393255e-05, 1181), (2.8393255e-05, 1180), (2.8393255e-05, 1179), (2.8393255e-05, 1178), (2.8393255e-05, 1177), (2.8393255e-05, 1176), (2.8393255e-05, 1175), (2.8393255e-05, 1174), (2.8393255e-05, 1173), (2.8393255e-05, 1172), (2.8393255e-05, 1171), (2.8393255e-05, 1170), (2.8393255e-05, 1169), (2.8393255e-05, 1168), (2.8393255e-05, 1167), (2.8393255e-05, 1166), (2.8393255e-05, 1165), (2.8393255e-05, 1164), (2.8393255e-05, 1163), (2.8393255e-05, 1162), (2.8393255e-05, 1161), (2.8393255e-05, 1160), (2.8393255e-05, 1159), (2.8393255e-05, 1158), (2.8393255e-05, 1157), (2.8393255e-05, 1156), (2.8393255e-05, 1155), (2.8393255e-05, 1154), (2.8393255e-05, 1153), (2.8393255e-05, 1152), (2.8393255e-05, 1151), (2.8393255e-05, 1150), (2.8393255e-05, 1149), (2.8393255e-05, 1148), (2.8393255e-05, 1147), (2.8393255e-05, 1146), (2.8393255e-05, 1145), (2.8393255e-05, 1144), (2.8393255e-05, 1143), (2.8393255e-05, 1142), (2.8393255e-05, 1141), (2.8393255e-05, 1140), (2.8393255e-05, 1139), (2.8393255e-05, 1138), (2.8393255e-05, 1137), (2.8393255e-05, 1136), (2.8393255e-05, 1135), (2.8393255e-05, 1134), (2.8393255e-05, 1133), (2.8393255e-05, 1132), (2.8393255e-05, 1131), (2.8393255e-05, 1130), (2.8393255e-05, 1129), (2.8393255e-05, 1128), (2.8393255e-05, 1127), (2.8393255e-05, 1126), (2.8393255e-05, 1125), (2.8393255e-05, 1124), (2.8393255e-05, 1123), (2.8393255e-05, 1122), (2.8393255e-05, 1121), (2.8393255e-05, 1120), (2.8393255e-05, 1119), (2.8393255e-05, 1118), (2.8393255e-05, 1117), (2.8393255e-05, 1116), (2.8393255e-05, 1115), (2.8393255e-05, 1114), (2.8393255e-05, 1113), (2.8393255e-05, 1112), (2.8393255e-05, 1111), (2.8393255e-05, 1110), (2.8393255e-05, 1109), (2.8393255e-05, 1108), (2.8393255e-05, 1107), (2.8393255e-05, 1106), (2.8393255e-05, 1105), (2.8393255e-05, 1104), (2.8393255e-05, 1103), (2.8393255e-05, 1102), (2.8393255e-05, 1101), (2.8393255e-05, 1100), (2.8393255e-05, 1099), (2.8393255e-05, 1098), (2.8393255e-05, 1097), (2.8393255e-05, 1096), (2.8393255e-05, 1095), (2.8393255e-05, 1094), (2.8393255e-05, 1093), (2.8393255e-05, 1092), (2.8393255e-05, 1091), (2.8393255e-05, 1090), (2.8393255e-05, 1089), (2.8393255e-05, 1088), (2.8393255e-05, 1087), (2.8393255e-05, 1086), (2.8393255e-05, 1085), (2.8393255e-05, 1084), (2.8393255e-05, 1083), (2.8393255e-05, 1082), (2.8393255e-05, 1081), (2.8393255e-05, 1080), (2.8393255e-05, 1079), (2.8393255e-05, 1078), (2.8393255e-05, 1077), (2.8393255e-05, 1076), (2.8393255e-05, 1075), (2.8393255e-05, 1074), (2.8393255e-05, 1073), (2.8393255e-05, 1072), (2.8393255e-05, 1071), (2.8393255e-05, 1070), (2.8393255e-05, 1069), (2.8393255e-05, 1068), (2.8393255e-05, 1067), (2.8393255e-05, 1066), (2.8393255e-05, 1065), (2.8393255e-05, 1064), (2.8393255e-05, 1063), (2.8393255e-05, 1062), (2.8393255e-05, 1061), (2.8393255e-05, 1060), (2.8393255e-05, 1059), (2.8393255e-05, 1058), (2.8393255e-05, 1057), (2.8393255e-05, 1056), (2.8393255e-05, 1055), (2.8393255e-05, 1054), (2.8393255e-05, 1053), (2.8393255e-05, 1052), (2.8393255e-05, 1051), (2.8393255e-05, 1050), (2.8393255e-05, 1049), (2.8393255e-05, 1048), (2.8393255e-05, 1047), (2.8393255e-05, 1046), (2.8393255e-05, 1045), (2.8393255e-05, 1044), (2.8393255e-05, 1043), (2.8393255e-05, 1042), (2.8393255e-05, 1041), (2.8393255e-05, 1040), (2.8393255e-05, 1039), (2.8393255e-05, 1038), (2.8393255e-05, 1037), (2.8393255e-05, 1036), (2.8393255e-05, 1035), (2.8393255e-05, 1034), (2.8393255e-05, 1033), (2.8393255e-05, 1030), (2.8393255e-05, 1029), (2.8393255e-05, 1028), (2.8393255e-05, 1027), (2.8393255e-05, 1026), (2.8393255e-05, 1025), (2.8393255e-05, 1024), (2.8393255e-05, 1023), (2.8393255e-05, 1022), (2.8393255e-05, 1021), (2.8393255e-05, 1020), (2.8393255e-05, 1019), (2.8393255e-05, 1018), (2.8393255e-05, 1017), (2.8393255e-05, 1016), (2.8393255e-05, 1015), (2.8393255e-05, 1014), (2.8393255e-05, 1013), (2.8393255e-05, 1012), (2.8393255e-05, 1011), (2.8393255e-05, 1010), (2.8393255e-05, 1009), (2.8393255e-05, 1008), (2.8393255e-05, 1007), (2.8393255e-05, 1006), (2.8393255e-05, 1005), (2.8393255e-05, 1004), (2.8393255e-05, 1003), (2.8393255e-05, 1002), (2.8393255e-05, 1001), (2.8393255e-05, 1000), (2.8393255e-05, 999), (2.8393255e-05, 998), (2.8393255e-05, 997), (2.8393255e-05, 996), (2.8393255e-05, 995), (2.8393255e-05, 994), (2.8393255e-05, 993), (2.8393255e-05, 992), (2.8393255e-05, 991), (2.8393255e-05, 990), (2.8393255e-05, 989), (2.8393255e-05, 988), (2.8393255e-05, 987), (2.8393255e-05, 986), (2.8393255e-05, 985), (2.8393255e-05, 984), (2.8393255e-05, 983), (2.8393255e-05, 982), (2.8393255e-05, 981), (2.8393255e-05, 980), (2.8393255e-05, 979), (2.8393255e-05, 978), (2.8393255e-05, 977), (2.8393255e-05, 976), (2.8393255e-05, 975), (2.8393255e-05, 974), (2.8393255e-05, 973), (2.8393255e-05, 972), (2.8393255e-05, 971), (2.8393255e-05, 970), (2.8393255e-05, 969), (2.8393255e-05, 968), (2.8393255e-05, 967), (2.8393255e-05, 966), (2.8393255e-05, 965), (2.8393255e-05, 964), (2.8393255e-05, 963), (2.8393255e-05, 962), (2.8393255e-05, 961), (2.8393255e-05, 960), (2.8393255e-05, 959), (2.8393255e-05, 958), (2.8393255e-05, 957), (2.8393255e-05, 956), (2.8393255e-05, 955), (2.8393255e-05, 954), (2.8393255e-05, 953), (2.8393255e-05, 952), (2.8393255e-05, 951), (2.8393255e-05, 950), (2.8393255e-05, 949), (2.8393255e-05, 948), (2.8393255e-05, 947), (2.8393255e-05, 946), (2.8393255e-05, 945), (2.8393255e-05, 944), (2.8393255e-05, 943), (2.8393255e-05, 942), (2.8393255e-05, 941), (2.8393255e-05, 940), (2.8393255e-05, 939), (2.8393255e-05, 938), (2.8393255e-05, 937), (2.8393255e-05, 936), (2.8393255e-05, 935), (2.8393255e-05, 934), (2.8393255e-05, 933), (2.8393255e-05, 932), (2.8393255e-05, 931), (2.8393255e-05, 930), (2.8393255e-05, 929), (2.8393255e-05, 928), (2.8393255e-05, 927), (2.8393255e-05, 926), (2.8393255e-05, 925), (2.8393255e-05, 924), (2.8393255e-05, 923), (2.8393255e-05, 922), (2.8393255e-05, 921), (2.8393255e-05, 920), (2.8393255e-05, 919), (2.8393255e-05, 918), (2.8393255e-05, 917), (2.8393255e-05, 916), (2.8393255e-05, 915), (2.8393255e-05, 914), (2.8393255e-05, 913), (2.8393255e-05, 912), (2.8393255e-05, 911), (2.8393255e-05, 910), (2.8393255e-05, 909), (2.8393255e-05, 908), (2.8393255e-05, 907), (2.8393255e-05, 906), (2.8393255e-05, 905), (2.8393255e-05, 904), (2.8393255e-05, 903), (2.8393255e-05, 902), (2.8393255e-05, 901), (2.8393255e-05, 900), (2.8393255e-05, 899), (2.8393255e-05, 898), (2.8393255e-05, 897), (2.8393255e-05, 896), (2.8393255e-05, 895), (2.8393255e-05, 894), (2.8393255e-05, 893), (2.8393255e-05, 892), (2.8393255e-05, 891), (2.8393255e-05, 890), (2.8393255e-05, 889), (2.8393255e-05, 888), (2.8393255e-05, 887), (2.8393255e-05, 886), (2.8393255e-05, 885), (2.8393255e-05, 884), (2.8393255e-05, 883), (2.8393255e-05, 882), (2.8393255e-05, 881), (2.8393255e-05, 880), (2.8393255e-05, 879), (2.8393255e-05, 878), (2.8393255e-05, 877), (2.8393255e-05, 876), (2.8393255e-05, 875), (2.8393255e-05, 874), (2.8393255e-05, 873), (2.8393255e-05, 872), (2.8393255e-05, 871), (2.8393255e-05, 870), (2.8393255e-05, 869), (2.8393255e-05, 868), (2.8393255e-05, 867), (2.8393255e-05, 866), (2.8393255e-05, 865), (2.8393255e-05, 864), (2.8393255e-05, 863), (2.8393255e-05, 862), (2.8393255e-05, 861), (2.8393255e-05, 860), (2.8393255e-05, 859), (2.8393255e-05, 858), (2.8393255e-05, 857), (2.8393255e-05, 856), (2.8393255e-05, 855), (2.8393255e-05, 854), (2.8393255e-05, 853), (2.8393255e-05, 852), (2.8393255e-05, 851), (2.8393255e-05, 850), (2.8393255e-05, 849), (2.8393255e-05, 848), (2.8393255e-05, 847), (2.8393255e-05, 846), (2.8393255e-05, 845), (2.8393255e-05, 844), (2.8393255e-05, 843), (2.8393255e-05, 842), (2.8393255e-05, 841), (2.8393255e-05, 840), (2.8393255e-05, 839), (2.8393255e-05, 838), (2.8393255e-05, 837), (2.8393255e-05, 836), (2.8393255e-05, 835), (2.8393255e-05, 834), (2.8393255e-05, 833), (2.8393255e-05, 832), (2.8393255e-05, 831), (2.8393255e-05, 830), (2.8393255e-05, 829), (2.8393255e-05, 828), (2.8393255e-05, 827), (2.8393255e-05, 826), (2.8393255e-05, 825), (2.8393255e-05, 824), (2.8393255e-05, 823), (2.8393255e-05, 822), (2.8393255e-05, 821), (2.8393255e-05, 820), (2.8393255e-05, 819), (2.8393255e-05, 818), (2.8393255e-05, 817), (2.8393255e-05, 816), (2.8393255e-05, 815), (2.8393255e-05, 814), (2.8393255e-05, 813), (2.8393255e-05, 812), (2.8393255e-05, 811), (2.8393255e-05, 810), (2.8393255e-05, 809), (2.8393255e-05, 808), (2.8393255e-05, 807), (2.8393255e-05, 806), (2.8393255e-05, 805), (2.8393255e-05, 804), (2.8393255e-05, 803), (2.8393255e-05, 802), (2.8393255e-05, 801), (2.8393255e-05, 800), (2.8393255e-05, 799), (2.8393255e-05, 798), (2.8393255e-05, 797), (2.8393255e-05, 796), (2.8393255e-05, 795), (2.8393255e-05, 794), (2.8393255e-05, 793), (2.8393255e-05, 792), (2.8393255e-05, 791), (2.8393255e-05, 790), (2.8393255e-05, 789), (2.8393255e-05, 788), (2.8393255e-05, 787), (2.8393255e-05, 786), (2.8393255e-05, 785), (2.8393255e-05, 784), (2.8393255e-05, 783), (2.8393255e-05, 782), (2.8393255e-05, 781), (2.8393255e-05, 780), (2.8393255e-05, 779), (2.8393255e-05, 778), (2.8393255e-05, 777), (2.8393255e-05, 776), (2.8393255e-05, 775), (2.8393255e-05, 774), (2.8393255e-05, 773), (2.8393255e-05, 772), (2.8393255e-05, 771), (2.8393255e-05, 770), (2.8393255e-05, 769), (2.8393255e-05, 768), (2.8393255e-05, 767), (2.8393255e-05, 766), (2.8393255e-05, 765), (2.8393255e-05, 764), (2.8393255e-05, 763), (2.8393255e-05, 760), (2.8393255e-05, 759), (2.8393255e-05, 758), (2.8393255e-05, 757), (2.8393255e-05, 756), (2.8393255e-05, 755), (2.8393255e-05, 754), (2.8393255e-05, 753), (2.8393255e-05, 752), (2.8393255e-05, 751), (2.8393255e-05, 750), (2.8393255e-05, 749), (2.8393255e-05, 748), (2.8393255e-05, 747), (2.8393255e-05, 746), (2.8393255e-05, 745), (2.8393255e-05, 744), (2.8393255e-05, 743), (2.8393255e-05, 742), (2.8393255e-05, 741), (2.8393255e-05, 740), (2.8393255e-05, 739), (2.8393255e-05, 738), (2.8393255e-05, 737), (2.8393255e-05, 736), (2.8393255e-05, 735), (2.8393255e-05, 734), (2.8393255e-05, 733), (2.8393255e-05, 732), (2.8393255e-05, 731), (2.8393255e-05, 730), (2.8393255e-05, 729), (2.8393255e-05, 728), (2.8393255e-05, 727), (2.8393255e-05, 726), (2.8393255e-05, 725), (2.8393255e-05, 724), (2.8393255e-05, 723), (2.8393255e-05, 722), (2.8393255e-05, 721), (2.8393255e-05, 720), (2.8393255e-05, 719), (2.8393255e-05, 718), (2.8393255e-05, 717), (2.8393255e-05, 716), (2.8393255e-05, 715), (2.8393255e-05, 714), (2.8393255e-05, 713), (2.8393255e-05, 712), (2.8393255e-05, 711), (2.8393255e-05, 710), (2.8393255e-05, 709), (2.8393255e-05, 708), (2.8393255e-05, 707), (2.8393255e-05, 706), (2.8393255e-05, 705), (2.8393255e-05, 704), (2.8393255e-05, 703), (2.8393255e-05, 702), (2.8393255e-05, 701), (2.8393255e-05, 700), (2.8393255e-05, 699), (2.8393255e-05, 698), (2.8393255e-05, 697), (2.8393255e-05, 696), (2.8393255e-05, 695), (2.8393255e-05, 694), (2.8393255e-05, 693), (2.8393255e-05, 692), (2.8393255e-05, 691), (2.8393255e-05, 690), (2.8393255e-05, 689), (2.8393255e-05, 688), (2.8393255e-05, 687), (2.8393255e-05, 686), (2.8393255e-05, 685), (2.8393255e-05, 684), (2.8393255e-05, 683), (2.8393255e-05, 682), (2.8393255e-05, 681), (2.8393255e-05, 680), (2.8393255e-05, 679), (2.8393255e-05, 678), (2.8393255e-05, 677), (2.8393255e-05, 676), (2.8393255e-05, 675), (2.8393255e-05, 674), (2.8393255e-05, 673), (2.8393255e-05, 672), (2.8393255e-05, 671), (2.8393255e-05, 670), (2.8393255e-05, 669), (2.8393255e-05, 668), (2.8393255e-05, 667), (2.8393255e-05, 666), (2.8393255e-05, 665), (2.8393255e-05, 664), (2.8393255e-05, 663), (2.8393255e-05, 662), (2.8393255e-05, 661), (2.8393255e-05, 660), (2.8393255e-05, 659), (2.8393255e-05, 658), (2.8393255e-05, 657), (2.8393255e-05, 656), (2.8393255e-05, 655), (2.8393255e-05, 654), (2.8393255e-05, 653), (2.8393255e-05, 652), (2.8393255e-05, 651), (2.8393255e-05, 650), (2.8393255e-05, 649), (2.8393255e-05, 648), (2.8393255e-05, 647), (2.8393255e-05, 646), (2.8393255e-05, 645), (2.8393255e-05, 644), (2.8393255e-05, 643), (2.8393255e-05, 642), (2.8393255e-05, 641), (2.8393255e-05, 640), (2.8393255e-05, 639), (2.8393255e-05, 638), (2.8393255e-05, 637), (2.8393255e-05, 636), (2.8393255e-05, 635), (2.8393255e-05, 634), (2.8393255e-05, 633), (2.8393255e-05, 632), (2.8393255e-05, 631), (2.8393255e-05, 630), (2.8393255e-05, 629), (2.8393255e-05, 628), (2.8393255e-05, 627), (2.8393255e-05, 626), (2.8393255e-05, 625), (2.8393255e-05, 624), (2.8393255e-05, 623), (2.8393255e-05, 622), (2.8393255e-05, 621), (2.8393255e-05, 620), (2.8393255e-05, 619), (2.8393255e-05, 618), (2.8393255e-05, 617), (2.8393255e-05, 616), (2.8393255e-05, 615), (2.8393255e-05, 614), (2.8393255e-05, 613), (2.8393255e-05, 612), (2.8393255e-05, 611), (2.8393255e-05, 610), (2.8393255e-05, 609), (2.8393255e-05, 608), (2.8393255e-05, 607), (2.8393255e-05, 606), (2.8393255e-05, 605), (2.8393255e-05, 604), (2.8393255e-05, 603), (2.8393255e-05, 602), (2.8393255e-05, 601), (2.8393255e-05, 600), (2.8393255e-05, 599), (2.8393255e-05, 598), (2.8393255e-05, 597), (2.8393255e-05, 596), (2.8393255e-05, 595), (2.8393255e-05, 594), (2.8393255e-05, 593), (2.8393255e-05, 592), (2.8393255e-05, 591), (2.8393255e-05, 590), (2.8393255e-05, 589), (2.8393255e-05, 588), (2.8393255e-05, 587), (2.8393255e-05, 586), (2.8393255e-05, 585), (2.8393255e-05, 584), (2.8393255e-05, 583), (2.8393255e-05, 582), (2.8393255e-05, 581), (2.8393255e-05, 580), (2.8393255e-05, 579), (2.8393255e-05, 578), (2.8393255e-05, 577), (2.8393255e-05, 576), (2.8393255e-05, 575), (2.8393255e-05, 574), (2.8393255e-05, 573), (2.8393255e-05, 572), (2.8393255e-05, 571), (2.8393255e-05, 570), (2.8393255e-05, 569), (2.8393255e-05, 568), (2.8393255e-05, 567), (2.8393255e-05, 566), (2.8393255e-05, 565), (2.8393255e-05, 564), (2.8393255e-05, 563), (2.8393255e-05, 562), (2.8393255e-05, 561), (2.8393255e-05, 560), (2.8393255e-05, 559), (2.8393255e-05, 558), (2.8393255e-05, 557), (2.8393255e-05, 556), (2.8393255e-05, 555), (2.8393255e-05, 554), (2.8393255e-05, 553), (2.8393255e-05, 552), (2.8393255e-05, 551), (2.8393255e-05, 550), (2.8393255e-05, 549), (2.8393255e-05, 548), (2.8393255e-05, 547), (2.8393255e-05, 546), (2.8393255e-05, 545), (2.8393255e-05, 544), (2.8393255e-05, 543), (2.8393255e-05, 542), (2.8393255e-05, 541), (2.8393255e-05, 540), (2.8393255e-05, 539), (2.8393255e-05, 538), (2.8393255e-05, 537), (2.8393255e-05, 536), (2.8393255e-05, 535), (2.8393255e-05, 534), (2.8393255e-05, 533), (2.8393255e-05, 532), (2.8393255e-05, 531), (2.8393255e-05, 530), (2.8393255e-05, 529), (2.8393255e-05, 528), (2.8393255e-05, 527), (2.8393255e-05, 526), (2.8393255e-05, 525), (2.8393255e-05, 524), (2.8393255e-05, 523), (2.8393255e-05, 522), (2.8393255e-05, 521), (2.8393255e-05, 520), (2.8393255e-05, 519), (2.8393255e-05, 518), (2.8393255e-05, 517), (2.8393255e-05, 516), (2.8393255e-05, 515), (2.8393255e-05, 514), (2.8393255e-05, 513), (2.8393255e-05, 512), (2.8393255e-05, 511), (2.8393255e-05, 510), (2.8393255e-05, 509), (2.8393255e-05, 508), (2.8393255e-05, 507), (2.8393255e-05, 506), (2.8393255e-05, 505), (2.8393255e-05, 504), (2.8393255e-05, 503), (2.8393255e-05, 502), (2.8393255e-05, 501), (2.8393255e-05, 500), (2.8393255e-05, 499), (2.8393255e-05, 498), (2.8393255e-05, 497), (2.8393255e-05, 494), (2.8393255e-05, 493), (2.8393255e-05, 492), (2.8393255e-05, 491), (2.8393255e-05, 490), (2.8393255e-05, 489), (2.8393255e-05, 488), (2.8393255e-05, 487), (2.8393255e-05, 486), (2.8393255e-05, 485), (2.8393255e-05, 484), (2.8393255e-05, 483), (2.8393255e-05, 482), (2.8393255e-05, 481), (2.8393255e-05, 480), (2.8393255e-05, 479), (2.8393255e-05, 478), (2.8393255e-05, 477), (2.8393255e-05, 476), (2.8393255e-05, 475), (2.8393255e-05, 474), (2.8393255e-05, 473), (2.8393255e-05, 472), (2.8393255e-05, 471), (2.8393255e-05, 470), (2.8393255e-05, 469), (2.8393255e-05, 468), (2.8393255e-05, 467), (2.8393255e-05, 466), (2.8393255e-05, 465), (2.8393255e-05, 464), (2.8393255e-05, 463), (2.8393255e-05, 462), (2.8393255e-05, 461), (2.8393255e-05, 460), (2.8393255e-05, 459), (2.8393255e-05, 458), (2.8393255e-05, 457), (2.8393255e-05, 456), (2.8393255e-05, 455), (2.8393255e-05, 454), (2.8393255e-05, 453), (2.8393255e-05, 452), (2.8393255e-05, 451), (2.8393255e-05, 450), (2.8393255e-05, 449), (2.8393255e-05, 448), (2.8393255e-05, 447), (2.8393255e-05, 446), (2.8393255e-05, 445), (2.8393255e-05, 444), (2.8393255e-05, 443), (2.8393255e-05, 442), (2.8393255e-05, 441), (2.8393255e-05, 440), (2.8393255e-05, 439), (2.8393255e-05, 438), (2.8393255e-05, 437), (2.8393255e-05, 436), (2.8393255e-05, 435), (2.8393255e-05, 434), (2.8393255e-05, 433), (2.8393255e-05, 432), (2.8393255e-05, 431), (2.8393255e-05, 430), (2.8393255e-05, 429), (2.8393255e-05, 428), (2.8393255e-05, 427), (2.8393255e-05, 426), (2.8393255e-05, 425), (2.8393255e-05, 424), (2.8393255e-05, 423), (2.8393255e-05, 422), (2.8393255e-05, 421), (2.8393255e-05, 420), (2.8393255e-05, 419), (2.8393255e-05, 418), (2.8393255e-05, 417), (2.8393255e-05, 416), (2.8393255e-05, 415), (2.8393255e-05, 414), (2.8393255e-05, 413), (2.8393255e-05, 412), (2.8393255e-05, 411), (2.8393255e-05, 410), (2.8393255e-05, 409), (2.8393255e-05, 408), (2.8393255e-05, 407), (2.8393255e-05, 406), (2.8393255e-05, 405), (2.8393255e-05, 404), (2.8393255e-05, 403), (2.8393255e-05, 402), (2.8393255e-05, 401), (2.8393255e-05, 400), (2.8393255e-05, 399), (2.8393255e-05, 398), (2.8393255e-05, 397), (2.8393255e-05, 396), (2.8393255e-05, 395), (2.8393255e-05, 394), (2.8393255e-05, 393), (2.8393255e-05, 392), (2.8393255e-05, 391), (2.8393255e-05, 390), (2.8393255e-05, 389), (2.8393255e-05, 388), (2.8393255e-05, 387), (2.8393255e-05, 386), (2.8393255e-05, 385), (2.8393255e-05, 384), (2.8393255e-05, 383), (2.8393255e-05, 382), (2.8393255e-05, 381), (2.8393255e-05, 380), (2.8393255e-05, 379), (2.8393255e-05, 378), (2.8393255e-05, 377), (2.8393255e-05, 376), (2.8393255e-05, 375), (2.8393255e-05, 374), (2.8393255e-05, 373), (2.8393255e-05, 372), (2.8393255e-05, 371), (2.8393255e-05, 370), (2.8393255e-05, 369), (2.8393255e-05, 368), (2.8393255e-05, 367), (2.8393255e-05, 366), (2.8393255e-05, 365), (2.8393255e-05, 364), (2.8393255e-05, 363), (2.8393255e-05, 362), (2.8393255e-05, 361), (2.8393255e-05, 360), (2.8393255e-05, 359), (2.8393255e-05, 358), (2.8393255e-05, 357), (2.8393255e-05, 356), (2.8393255e-05, 355), (2.8393255e-05, 354), (2.8393255e-05, 353), (2.8393255e-05, 352), (2.8393255e-05, 351), (2.8393255e-05, 350), (2.8393255e-05, 349), (2.8393255e-05, 348), (2.8393255e-05, 347), (2.8393255e-05, 346), (2.8393255e-05, 345), (2.8393255e-05, 344), (2.8393255e-05, 343), (2.8393255e-05, 342), (2.8393255e-05, 341), (2.8393255e-05, 340), (2.8393255e-05, 339), (2.8393255e-05, 338), (2.8393255e-05, 337), (2.8393255e-05, 336), (2.8393255e-05, 335), (2.8393255e-05, 334), (2.8393255e-05, 333), (2.8393255e-05, 332), (2.8393255e-05, 331), (2.8393255e-05, 330), (2.8393255e-05, 329), (2.8393255e-05, 328), (2.8393255e-05, 327), (2.8393255e-05, 326), (2.8393255e-05, 325), (2.8393255e-05, 324), (2.8393255e-05, 323), (2.8393255e-05, 322), (2.8393255e-05, 321), (2.8393255e-05, 320), (2.8393255e-05, 319), (2.8393255e-05, 318), (2.8393255e-05, 317), (2.8393255e-05, 316), (2.8393255e-05, 315), (2.8393255e-05, 314), (2.8393255e-05, 313), (2.8393255e-05, 312), (2.8393255e-05, 311), (2.8393255e-05, 310), (2.8393255e-05, 309), (2.8393255e-05, 308), (2.8393255e-05, 307), (2.8393255e-05, 306), (2.8393255e-05, 305), (2.8393255e-05, 304), (2.8393255e-05, 303), (2.8393255e-05, 302), (2.8393255e-05, 301), (2.8393255e-05, 300), (2.8393255e-05, 299), (2.8393255e-05, 298), (2.8393255e-05, 297), (2.8393255e-05, 296), (2.8393255e-05, 295), (2.8393255e-05, 294), (2.8393255e-05, 293), (2.8393255e-05, 292), (2.8393255e-05, 291), (2.8393255e-05, 290), (2.8393255e-05, 289), (2.8393255e-05, 288), (2.8393255e-05, 287), (2.8393255e-05, 286), (2.8393255e-05, 285), (2.8393255e-05, 284), (2.8393255e-05, 283), (2.8393255e-05, 282), (2.8393255e-05, 281), (2.8393255e-05, 280), (2.8393255e-05, 279), (2.8393255e-05, 278), (2.8393255e-05, 277), (2.8393255e-05, 276), (2.8393255e-05, 275), (2.8393255e-05, 274), (2.8393255e-05, 273), (2.8393255e-05, 272), (2.8393255e-05, 271), (2.8393255e-05, 270), (2.8393255e-05, 269), (2.8393255e-05, 268), (2.8393255e-05, 267), (2.8393255e-05, 266), (2.8393255e-05, 265), (2.8393255e-05, 264), (2.8393255e-05, 263), (2.8393255e-05, 262), (2.8393255e-05, 261), (2.8393255e-05, 260), (2.8393255e-05, 259), (2.8393255e-05, 258), (2.8393255e-05, 257), (2.8393255e-05, 256), (2.8393255e-05, 255), (2.8393255e-05, 254), (2.8393255e-05, 253), (2.8393255e-05, 252), (2.8393255e-05, 251), (2.8393255e-05, 250), (2.8393255e-05, 247), (2.8393255e-05, 246), (2.8393255e-05, 245), (2.8393255e-05, 244), (2.8393255e-05, 243), (2.8393255e-05, 242), (2.8393255e-05, 241), (2.8393255e-05, 240), (2.8393255e-05, 239), (2.8393255e-05, 238), (2.8393255e-05, 237), (2.8393255e-05, 236), (2.8393255e-05, 235), (2.8393255e-05, 234), (2.8393255e-05, 233), (2.8393255e-05, 232), (2.8393255e-05, 231), (2.8393255e-05, 230), (2.8393255e-05, 229), (2.8393255e-05, 228), (2.8393255e-05, 227), (2.8393255e-05, 226), (2.8393255e-05, 225), (2.8393255e-05, 224), (2.8393255e-05, 222), (2.8393255e-05, 221), (2.8393255e-05, 220), (2.8393255e-05, 219), (2.8393255e-05, 218), (2.8393255e-05, 217), (2.8393255e-05, 216), (2.8393255e-05, 215), (2.8393255e-05, 214), (2.8393255e-05, 212), (2.8393255e-05, 211), (2.8393255e-05, 210), (2.8393255e-05, 209), (2.8393255e-05, 208), (2.8393255e-05, 207), (2.8393255e-05, 206), (2.8393255e-05, 205), (2.8393255e-05, 204), (2.8393255e-05, 203), (2.8393255e-05, 202), (2.8393255e-05, 201), (2.8393255e-05, 200), (2.8393255e-05, 199), (2.8393255e-05, 198), (2.8393255e-05, 197), (2.8393255e-05, 196), (2.8393255e-05, 195), (2.8393255e-05, 194), (2.8393255e-05, 193), (2.8393255e-05, 192), (2.8393255e-05, 191), (2.8393255e-05, 190), (2.8393255e-05, 189), (2.8393255e-05, 188), (2.8393255e-05, 187), (2.8393255e-05, 186), (2.8393255e-05, 185), (2.8393255e-05, 184), (2.8393255e-05, 183), (2.8393255e-05, 182), (2.8393255e-05, 181), (2.8393255e-05, 180), (2.8393255e-05, 179), (2.8393255e-05, 178), (2.8393255e-05, 177), (2.8393255e-05, 176), (2.8393255e-05, 175), (2.8393255e-05, 174), (2.8393255e-05, 173), (2.8393255e-05, 172), (2.8393255e-05, 171), (2.8393255e-05, 170), (2.8393255e-05, 169), (2.8393255e-05, 168), (2.8393255e-05, 167), (2.8393255e-05, 166), (2.8393255e-05, 165), (2.8393255e-05, 164), (2.8393255e-05, 163), (2.8393255e-05, 162), (2.8393255e-05, 161), (2.8393255e-05, 160), (2.8393255e-05, 159), (2.8393255e-05, 158), (2.8393255e-05, 157), (2.8393255e-05, 156), (2.8393255e-05, 155), (2.8393255e-05, 154), (2.8393255e-05, 153), (2.8393255e-05, 152), (2.8393255e-05, 151), (2.8393255e-05, 150), (2.8393255e-05, 149), (2.8393255e-05, 148), (2.8393255e-05, 147), (2.8393255e-05, 146), (2.8393255e-05, 145), (2.8393255e-05, 144), (2.8393255e-05, 143), (2.8393255e-05, 142), (2.8393255e-05, 141), (2.8393255e-05, 140), (2.8393255e-05, 139), (2.8393255e-05, 138), (2.8393255e-05, 137), (2.8393255e-05, 136), (2.8393255e-05, 135), (2.8393255e-05, 134), (2.8393255e-05, 133), (2.8393255e-05, 132), (2.8393255e-05, 131), (2.8393255e-05, 130), (2.8393255e-05, 129), (2.8393255e-05, 128), (2.8393255e-05, 127), (2.8393255e-05, 126), (2.8393255e-05, 125), (2.8393255e-05, 124), (2.8393255e-05, 123), (2.8393255e-05, 122), (2.8393255e-05, 121), (2.8393255e-05, 120), (2.8393255e-05, 119), (2.8393255e-05, 118), (2.8393255e-05, 117), (2.8393255e-05, 116), (2.8393255e-05, 115), (2.8393255e-05, 114), (2.8393255e-05, 113), (2.8393255e-05, 112), (2.8393255e-05, 111), (2.8393255e-05, 110), (2.8393255e-05, 109), (2.8393255e-05, 108), (2.8393255e-05, 107), (2.8393255e-05, 106), (2.8393255e-05, 105), (2.8393255e-05, 104), (2.8393255e-05, 103), (2.8393255e-05, 102), (2.8393255e-05, 101), (2.8393255e-05, 100), (2.8393255e-05, 99), (2.8393255e-05, 98), (2.8393255e-05, 97), (2.8393255e-05, 96), (2.8393255e-05, 95), (2.8393255e-05, 94), (2.8393255e-05, 93), (2.8393255e-05, 92), (2.8393255e-05, 91), (2.8393255e-05, 90), (2.8393255e-05, 89), (2.8393255e-05, 88), (2.8393255e-05, 87), (2.8393255e-05, 86), (2.8393255e-05, 85), (2.8393255e-05, 84), (2.8393255e-05, 83), (2.8393255e-05, 82), (2.8393255e-05, 81), (2.8393255e-05, 80), (2.8393255e-05, 79), (2.8393255e-05, 78), (2.8393255e-05, 77), (2.8393255e-05, 76), (2.8393255e-05, 75), (2.8393255e-05, 74), (2.8393255e-05, 73), (2.8393255e-05, 72), (2.8393255e-05, 71), (2.8393255e-05, 70), (2.8393255e-05, 69), (2.8393255e-05, 68), (2.8393255e-05, 67), (2.8393255e-05, 66), (2.8393255e-05, 65), (2.8393255e-05, 64), (2.8393255e-05, 63), (2.8393255e-05, 62), (2.8393255e-05, 61), (2.8393255e-05, 60), (2.8393255e-05, 59), (2.8393255e-05, 58), (2.8393255e-05, 57), (2.8393255e-05, 56), (2.8393255e-05, 55), (2.8393255e-05, 54), (2.8393255e-05, 53), (2.8393255e-05, 52), (2.8393255e-05, 51), (2.8393255e-05, 50), (2.8393255e-05, 49), (2.8393255e-05, 48), (2.8393255e-05, 47), (2.8393255e-05, 46), (2.8393255e-05, 45), (2.8393255e-05, 44), (2.8393255e-05, 43), (2.8393255e-05, 42), (2.8393255e-05, 41), (2.8393255e-05, 40), (2.8393255e-05, 39), (2.8393255e-05, 38), (2.8393255e-05, 37), (2.8393255e-05, 36), (2.8393255e-05, 35), (2.8393255e-05, 34), (2.8393255e-05, 33), (2.8393255e-05, 32), (2.8393255e-05, 31), (2.8393255e-05, 28), (2.8393255e-05, 27), (2.8393255e-05, 26), (2.8393255e-05, 25), (2.8393255e-05, 24), (2.8393255e-05, 23), (2.8393255e-05, 22), (2.8393255e-05, 21), (2.8393255e-05, 20), (2.8393255e-05, 19), (2.8393255e-05, 18), (2.8393255e-05, 17), (2.8393255e-05, 16), (2.8393255e-05, 15), (2.8393255e-05, 14), (2.8393255e-05, 13), (2.8393255e-05, 12), (2.8393255e-05, 11), (2.8393255e-05, 10), (2.8393255e-05, 9), (2.8393255e-05, 8), (2.8393255e-05, 7), (2.8393255e-05, 6), (2.8393255e-05, 5), (2.8393255e-05, 4), (2.8393255e-05, 3), (2.8393255e-05, 2), (2.8393255e-05, 1), (2.8393255e-05, 0), (2.5625717e-05, 1566), (3.1935224e-06, 1809), (2.0889515e-06, 1540)]\n",
      "d2d4 0.63404244\n",
      "e2e4 0.30915236\n",
      "f2f4 0.00054985494\n",
      "a2a3 0.00013388555\n",
      "f2f3 0.00010157549\n",
      "e2e3 9.4130475e-05\n",
      "c2c3 6.9529495e-05\n",
      "b1c3 6.3696054e-05\n",
      "d2d3 5.403922e-05\n",
      "g2g4 5.3789186e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d2d4'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_output(model, move_list, tokenizer: ChessTokenizer, seq_len=256):\n",
    "    board = chess.Board()\n",
    "\n",
    "    token_ids = [0] * seq_len\n",
    "    attn_mask = [0] * seq_len\n",
    "\n",
    "    for i in range(len(move_list)):\n",
    "        move = move_list[i]\n",
    "        new_move = chess.Move.from_uci(move)\n",
    "        board.push(new_move)\n",
    "        token_ids[i] = tokenizer.tokens_to_ids_single(move)\n",
    "        attn_mask[i] = 1\n",
    "\n",
    "\n",
    "    legal_moves = list(board.legal_moves)\n",
    "    legal_moves = [str(move) for move in legal_moves]\n",
    "    legal_mask = [0] * tokenizer.vocabulary_size()\n",
    "\n",
    "    legal_moves_ids = tokenizer.tokens_to_ids_vect(legal_moves)\n",
    "\n",
    "    for id in legal_moves_ids:\n",
    "        legal_mask[id] = 1\n",
    "    \n",
    "    token_ids = torch.tensor(token_ids)\n",
    "    attn_mask = torch.tensor(attn_mask)\n",
    "    legal_mask = torch.tensor(legal_mask)\n",
    "\n",
    "    token_ids = torch.reshape(token_ids, (1, -1))\n",
    "    attn_mask = torch.reshape(attn_mask, (1, -1))\n",
    "    legal_mask = torch.reshape(legal_mask, (1, -1))\n",
    "\n",
    "    token_ids = token_ids.to(mps_device)\n",
    "    attn_mask = attn_mask.to(mps_device)\n",
    "    legal_mask = legal_mask.to(mps_device)\n",
    "\n",
    "    outputs = model(token_ids, attn_mask, legal_mask)\n",
    "\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    outputs = softmax(outputs)\n",
    "\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    outputs = outputs[0]\n",
    "\n",
    "    to_sort = [(outputs[i], i) for i in range(len(outputs))]\n",
    "    to_sort.sort(reverse=True)\n",
    "\n",
    "    print(to_sort)\n",
    "\n",
    "    for i in range(10):\n",
    "        print(tokenizer.ids_to_tokens_single(to_sort[i][1]), to_sort[i][0])\n",
    "\n",
    "    return tokenizer.ids_to_tokens_single(to_sort[0][1])\n",
    "\n",
    "\n",
    "\n",
    "get_model_output(model, [\"e2e4\", \"e7e5\", \"f1c4\", \"b8c6\", \"b1c3\", \"g7g6\", \"g1f3\", \"d7d5\", \"d2d3\"], tok, seq_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
